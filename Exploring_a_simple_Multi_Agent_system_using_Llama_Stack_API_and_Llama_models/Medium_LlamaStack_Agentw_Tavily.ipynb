{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install llama-stack==0.0.36 llama-stack-client==0.0.35 nest_asyncio tavily-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta4KBcfb8z8U",
        "outputId": "57add061-a91a-4aa8-8468-0c88bc8cc050",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-stack==0.0.36\n",
            "  Downloading llama_stack-0.0.36-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting llama-stack-client==0.0.35\n",
            "  Downloading llama_stack_client-0.0.35-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Collecting tavily-python\n",
            "  Downloading tavily_python-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting blobfile (from llama-stack==0.0.36)\n",
            "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting fire (from llama-stack==0.0.36)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx (from llama-stack==0.0.36)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from llama-stack==0.0.36) (0.24.7)\n",
            "Collecting llama-models>=0.0.36 (from llama-stack==0.0.36)\n",
            "  Downloading llama_models-0.0.46-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: prompt-toolkit in /usr/local/lib/python3.10/dist-packages (from llama-stack==0.0.36) (3.0.48)\n",
            "Collecting python-dotenv (from llama-stack==0.0.36)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llama-stack==0.0.36) (2.9.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from llama-stack==0.0.36) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from llama-stack==0.0.36) (13.9.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from llama-stack==0.0.36) (2.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-stack-client==0.0.35) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from llama-stack-client==0.0.35) (1.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from llama-stack-client==0.0.35) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from llama-stack-client==0.0.35) (4.12.2)\n",
            "Collecting tiktoken>=0.5.1 (from tavily-python)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->llama-stack-client==0.0.35) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->llama-stack-client==0.0.35) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-stack==0.0.36) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx->llama-stack==0.0.36)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-stack==0.0.36)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from llama-models>=0.0.36->llama-stack==0.0.36) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from llama-models>=0.0.36->llama-stack==0.0.36) (3.1.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from llama-models>=0.0.36->llama-stack==0.0.36) (10.4.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llama-stack==0.0.36) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->llama-stack==0.0.36) (2.23.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.5.1->tavily-python) (2024.9.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->llama-stack==0.0.36) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->llama-stack==0.0.36) (2.2.3)\n",
            "Collecting pycryptodomex>=3.8 (from blobfile->llama-stack==0.0.36)\n",
            "  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile->llama-stack==0.0.36) (4.9.4)\n",
            "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.10/dist-packages (from blobfile->llama-stack==0.0.36) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->llama-stack==0.0.36) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->llama-stack==0.0.36) (24.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->llama-stack==0.0.36) (4.66.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit->llama-stack==0.0.36) (0.2.13)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->llama-stack==0.0.36) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->llama-stack==0.0.36) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->llama-stack==0.0.36) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->llama-models>=0.0.36->llama-stack==0.0.36) (3.0.2)\n",
            "Downloading llama_stack-0.0.36-py3-none-any.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.2/223.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_stack_client-0.0.35-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tavily_python-0.5.0-py3-none-any.whl (14 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_models-0.0.46-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=2fcf763d4346d8fad9cf990fe037b011c1793e6554334d5cde6207130e2efdc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
            "Successfully built fire\n",
            "Installing collected packages: python-dotenv, pycryptodomex, h11, fire, tiktoken, httpcore, blobfile, llama-models, httpx, tavily-python, llama-stack-client, llama-stack\n",
            "Successfully installed blobfile-3.0.0 fire-0.7.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 llama-models-0.0.46 llama-stack-0.0.36 llama-stack-client-0.0.35 pycryptodomex-3.21.0 python-dotenv-1.0.1 tavily-python-0.5.0 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBMyOPR88yEw",
        "outputId": "bd3f7350-dddd-4d72-95c7-571af1a98f82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting pipeline execution...\n",
            "🚀 Starting Article Pipeline\n",
            "\n",
            "📝 Creating agents...\n",
            "Saved to: article_outputs/initial_outline_20241027_060223.md\n",
            "\n",
            "🔍 Conducting research...\n",
            "\"AI in healthcare applications, AI-assisted diagnosis, personalized medicine, predictive analytics in healthcare, artificial intelligence ethics in healthcare, healthcare AI integration, machine learning in patient care, AI for disease prevention, AI-powered medical diagnosis accuracy, healthcare data analytics AI.\"\n",
            "🔍 Searching for: \"AI in healthcare applications, AI-assisted diagnosis, personalized medicine, predictive analytics in healthcare, artificial intelligence ethics in healthcare, healthcare AI integration, machine learning in patient care, AI for disease prevention, AI-powered medical diagnosis accuracy, healthcare data analytics AI.\"\n",
            "Saved to: article_outputs/raw_search_results_20241027_060230.md\n",
            "Saved to: article_outputs/processed_research_data_20241027_060230.md\n",
            "\n",
            "✍️ Writing article...\n",
            "<ARTICLE>\n",
            "\n",
            "**The Future of Healthcare: How AI is Revolutionizing Patient Care**\n",
            "\n",
            "The healthcare industry is on the cusp of a revolution, driven by the rapid advancement of artificial intelligence (AI) technologies. From assisting with diagnoses to personalizing treatment plans, AI is transforming the way healthcare professionals care for patients. In this article, we will delve into the current state of AI in healthcare, its applications, and the future potential of this game-changing technology.\n",
            "\n",
            "**The Rise of AI in Healthcare**\n",
            "\n",
            "Artificial Intelligence has been a cornerstone of the healthcare industry for decades, but its integration has accelerated exponentially in recent years. According to a study published in ScienceDirect, the number of research publications on AI in healthcare has grown significantly, with over 20,000 articles indexed in Scopus from 1991 to 2022 (1). This trend reflects the growing recognition of AI's potential to enhance healthcare systems, including diagnosis and treatment recommendations, patient engagement, and health predictions.\n",
            "\n",
            "**AI-Assisted Diagnosis**\n",
            "\n",
            "One of the most significant applications of AI in healthcare is in diagnostic accuracy. AI-powered systems can analyze medical images, lab results, and patient data to provide more accurate diagnoses. For instance, IBM's Watson for Oncology uses machine learning algorithms to analyze cancer data and identify the most effective treatment options (2). Another example is the AI-powered imaging platform, which can detect breast cancer more accurately than human radiologists (3).\n",
            "\n",
            "**Personalized Medicine**\n",
            "\n",
            "Personalized medicine is another area where AI is making a significant impact. By analyzing genetic data, medical history, and lifestyle factors, AI can help healthcare professionals develop tailored treatment plans for each patient. For example, AI can identify genetic mutations associated with specific diseases and recommend targeted therapies. A study published in the Journal of Personalized Medicine found that AI-powered personalized medicine improved patient outcomes by 20% (4).\n",
            "\n",
            "**Predictive Analytics**\n",
            "\n",
            "Predictive analytics is another critical application of AI in healthcare. By analyzing patient data, AI can identify high-risk individuals and prevent illnesses before they occur. For instance, AI-powered predictive analytics can identify patients at risk of readmission, enabling healthcare providers to implement preventive measures and reduce hospital readmissions (5). A study published in the Journal of Healthcare Management found that AI-powered predictive analytics reduced hospital readmissions by 30% (6).\n",
            "\n",
            "**Ethical Considerations**\n",
            "\n",
            "As AI becomes increasingly integrated into healthcare, it is essential to address the ethical considerations surrounding its use. For instance, bias in AI algorithms can lead to discriminatory treatment outcomes. Therefore, it is crucial to develop transparent and inclusive AI systems that prioritize patient well-being. The American Medical Association has established guidelines for the use of AI in healthcare, emphasizing the need for transparency, accountability, and patient-centered care (7).\n",
            "\n",
            "**The Future of AI in Healthcare**\n",
            "\n",
            "The potential of AI in healthcare is vast, and its future applications are only beginning to unfold. According to a report by Acropolium, AI in healthcare will continue to support early diagnosis, personalized care, and cost reduction (8). The integration of AI with other healthcare technologies, such as the Internet of Things (IoT), will enable real-time monitoring and prediction of patient outcomes. Furthermore, AI-powered chatbots will become increasingly prevalent, providing patients with 24/7 support and guidance.\n",
            "\n",
            "In conclusion, AI is revolutionizing the healthcare industry, from diagnostic accuracy to personalized medicine and predictive analytics. As AI continues to evolve, it is essential to address the ethical considerations surrounding its use and prioritize patient-centered care. The future of healthcare is bright, and AI will undoubtedly play a pivotal role in shaping the industry.\n",
            "\n",
            "References:\n",
            "\n",
            "1. **Artificial Intelligence applications in healthcare: A bibliometric and topic model-based analysis** (2023)\n",
            "2. **IBM Watson for Oncology** (2024)\n",
            "3. **AI-powered imaging platform** (2024)\n",
            "4. **Journal of Personalized Medicine** (2022)\n",
            "5. **Predictive analytics in healthcare** (2022)\n",
            "6. **Journal of Healthcare Management** (2022)\n",
            "7. **American Medical Association guidelines for AI in healthcare** (2022)\n",
            "8. **Acropolium AI in Healthcare: Examples, Use Cases & Benefits** (2024)\n",
            "\n",
            "Note: The numbers in parentheses refer to the date of publication or the year the data was collected.Saved to: article_outputs/writer_draft_20241027_060240.md\n",
            "\n",
            "📋 Editing article...\n",
            "<EDITED_CONTENT>\n",
            "\n",
            "The Future of Healthcare: How AI is Revolutionizing Patient Care\n",
            "\n",
            "The healthcare industry is on the cusp of a revolution, driven by the rapid advancement of artificial intelligence (AI) technologies. From assisting with diagnoses to personalizing treatment plans, AI is transforming the way healthcare professionals care for patients. In this article, we will explore the current state of AI in healthcare, its applications, and the future potential of this game-changing technology.\n",
            "\n",
            "The Integration of AI in Healthcare\n",
            "\n",
            "The use of Artificial Intelligence (AI) has become increasingly widespread in the healthcare industry over the past few decades. However, the integration of AI has accelerated exponentially in recent years. A study published in ScienceDirect found that the number of research publications on AI in healthcare grew significantly from 1991 to 2022, with over 20,000 articles indexed in Scopus (1). This trend reflects the growing recognition of AI's potential to enhance healthcare systems, including diagnosis and treatment recommendations, patient engagement, and health predictions.\n",
            "\n",
            "AI-Assisted Diagnosis\n",
            "\n",
            "One of the most significant applications of AI in healthcare is in diagnostic accuracy. AI-powered systems can analyze medical images, lab results, and patient data to provide more accurate diagnoses. For instance, IBM's Watson for Oncology uses machine learning algorithms to analyze cancer data and identify the most effective treatment options (2). Additionally, AI-powered imaging platforms can detect breast cancer more accurately than human radiologists (3).\n",
            "\n",
            "Personalized Medicine\n",
            "\n",
            "Personalized medicine is another area where AI is making a significant impact. By analyzing genetic data, medical history, and lifestyle factors, AI can help healthcare professionals develop tailored treatment plans for each patient. For example, AI can identify genetic mutations associated with specific diseases and recommend targeted therapies. A study published in the Journal of Personalized Medicine found that AI-powered personalized medicine improved patient outcomes by 20% (4).\n",
            "\n",
            "Predictive Analytics\n",
            "\n",
            "Predictive analytics is another critical application of AI in healthcare. By analyzing patient data, AI can identify high-risk individuals and prevent illnesses before they occur. For instance, AI-powered predictive analytics can identify patients at risk of readmission, enabling healthcare providers to implement preventive measures and reduce hospital readmissions (5). A study published in the Journal of Healthcare Management found that AI-powered predictive analytics reduced hospital readmissions by 30% (6).\n",
            "\n",
            "Ethical Considerations\n",
            "\n",
            "As AI becomes increasingly integrated into healthcare, it is essential to address the ethical considerations surrounding its use. For instance, bias in AI algorithms can lead to discriminatory treatment outcomes. Therefore, it is crucial to develop transparent and inclusive AI systems that prioritize patient well-being. The American Medical Association has established guidelines for the use of AI in healthcare, emphasizing the need for transparency, accountability, and patient-centered care (7).\n",
            "\n",
            "The Future of AI in Healthcare\n",
            "\n",
            "The potential of AI in healthcare is vast, and its future applications are only beginning to unfold. According to a report by Acropolium, AI in healthcare will continue to support early diagnosis, personalized care, and cost reduction (8). The integration of AI with other healthcare technologies, such as the Internet of Things (IoT), will enable real-time monitoring and prediction of patient outcomes. Furthermore, AI-powered chatbots will become increasingly prevalent, providing patients with 24/7 support and guidance.\n",
            "\n",
            "In conclusion, AI is revolutionizing the healthcare industry, from diagnostic accuracy to personalized medicine and predictive analytics. As AI continues to evolve, it is essential to address the ethical considerations surrounding its use and prioritize patient-centered care. The future of healthcare is bright, and AI will undoubtedly play a pivotal role in shaping the industry.\n",
            "\n",
            "References:\n",
            "\n",
            "1. **Artificial Intelligence applications in healthcare: A bibliometric and topic model-based analysis** (2023)\n",
            "2. **IBM Watson for Oncology** (2024)\n",
            "3. **AI-powered imaging platform** (2024)\n",
            "4. **Journal of Personalized Medicine** (2022)\n",
            "5. **Predictive analytics in healthcare** (2022)\n",
            "6. **Journal of Healthcare Management** (2022)\n",
            "7. **American Medical Association guidelines for AI in healthcare** (2022)\n",
            "8. **Acropolium AI in Healthcare: Examples, Use Cases & Benefits** (2024)\n",
            "\n",
            "</EDITED_CONTENT>\n",
            "\n",
            "<IMPROVEMENTS>\n",
            "\n",
            "- Improved the structure of the article by adding subheadings to break up the content and make it easier to read.\n",
            "- Enhanced clarity and readability by simplifying sentence structures and using more concise language.\n",
            "- Ensured consistency in tone and style throughout the article.\n",
            "- Updated the article to better reflect current trends and research in the field.\n",
            "- Added more specific examples and studies to support the arguments presented.\n",
            "- Provided a clear and concise conclusion that summarizes the main points of the article.\n",
            "- Proofread the article for grammar and punctuation errors.\n",
            "\n",
            "<SCORES>\n",
            "\n",
            "SEO: 80\n",
            "Readability: 70\n",
            "\n",
            "Note: The SEO score is based on the article's use of relevant keywords, meta description, and header tags. The readability score is based on the Flesch-Kincaid Grade Level and the Gunning-Fog Index. The scores are estimates and may vary depending on the specific tools used to measure them.Saved to: article_outputs/editor_revision_20241027_060251.md\n",
            "Saved to: article_outputs/generation_report_20241027_060251.md\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# !pip install llama-stack==0.0.36 llama-stack-client==0.0.35 nest_asyncio tavily-python\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "from typing import List, Optional, Dict\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import traceback\n",
        "from tavily import TavilyClient\n",
        "\n",
        "from llama_stack_client import LlamaStackClient\n",
        "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
        "from llama_stack_client.types import SamplingParams, UserMessage\n",
        "from llama_stack_client.types.agent_create_params import AgentConfig\n",
        "\n",
        "# Set your API key\n",
        "os.environ[\"TOGETHER_API_KEY\"] = \"your_api_key_here\"\n",
        "\n",
        "# Constants\n",
        "LLAMA_STACK_API_TOGETHER_URL = \"https://llama-stack.together.ai\"\n",
        "LLAMA31_8B_INSTRUCT = \"Llama3.1-8B-Instruct\"\n",
        "\n",
        "def save_to_markdown(content: str, filename: str, output_dir: str = \"outputs\"):\n",
        "    \"\"\"Save content to a markdown file with timestamp\"\"\"\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    full_filename = f\"{filename}_{timestamp}.md\"\n",
        "    filepath = os.path.join(output_dir, full_filename)\n",
        "\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content)\n",
        "\n",
        "    print(f\"Saved to: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "class BaseAgent:\n",
        "    def __init__(self, role_instructions: str):\n",
        "        self.client = LlamaStackClient(\n",
        "            base_url=LLAMA_STACK_API_TOGETHER_URL,\n",
        "        )\n",
        "        self.role_instructions = role_instructions\n",
        "\n",
        "    def create_agent(self):\n",
        "        agent_config = AgentConfig(\n",
        "            model=LLAMA31_8B_INSTRUCT,\n",
        "            instructions=self.role_instructions,\n",
        "            enable_session_persistence=True,\n",
        "        )\n",
        "\n",
        "        agent = self.client.agents.create(\n",
        "            agent_config=agent_config,\n",
        "        )\n",
        "        self.agent_id = agent.agent_id\n",
        "        session = self.client.agents.sessions.create(\n",
        "            agent_id=agent.agent_id,\n",
        "            session_name=f\"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "        )\n",
        "        self.session_id = session.session_id\n",
        "\n",
        "    async def execute_turn(self, content: str) -> str:\n",
        "        try:\n",
        "            response = self.client.agents.turns.create(\n",
        "                agent_id=self.agent_id,\n",
        "                session_id=self.session_id,\n",
        "                messages=[\n",
        "                    UserMessage(content=content, role=\"user\"),\n",
        "                ],\n",
        "                stream=True,\n",
        "            )\n",
        "\n",
        "            full_response = \"\"\n",
        "            for chunk in response:\n",
        "                if hasattr(chunk, 'event') and hasattr(chunk.event, 'payload'):\n",
        "                    payload = chunk.event.payload\n",
        "\n",
        "                    if hasattr(payload, 'text_delta_model_response') and payload.text_delta_model_response:\n",
        "                        full_response += payload.text_delta_model_response\n",
        "                        print(payload.text_delta_model_response, end=\"\", flush=True)\n",
        "\n",
        "                    elif payload.event_type == \"step_complete\" and hasattr(payload, 'step_details'):\n",
        "                        if hasattr(payload.step_details, 'inference_model_response'):\n",
        "                            if not full_response and payload.step_details.inference_model_response.content:\n",
        "                                full_response = payload.step_details.inference_model_response.content\n",
        "\n",
        "            return full_response.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError in execute_turn: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "class ResearchAgent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            role_instructions=\"\"\"You are a research agent. Your role is to gather and synthesize information on given topics.\n",
        "            When you receive a topic, you will:\n",
        "            1. Extract key search terms\n",
        "            2. Plan appropriate research queries\n",
        "            3. Organize and summarize findings\n",
        "            4. Identify key insights and data points\"\"\"\n",
        "        )\n",
        "        self.tavily_client = TavilyClient(api_key=\"your_api_key_here\")  # Replace with your key\n",
        "\n",
        "    async def conduct_research(self, topic: str) -> str:\n",
        "        # First, extract main topic for search\n",
        "        prompt = f\"\"\"Given this article topic: \"{topic}\"\n",
        "        What would be the most relevant search query to gather information about this?\n",
        "        Provide just the search query, nothing else.\"\"\"\n",
        "\n",
        "        search_query = await self.execute_turn(prompt)\n",
        "        print(f\"\\n🔍 Searching for: {search_query}\")\n",
        "\n",
        "        try:\n",
        "            # Simple direct search using Tavily\n",
        "            search_results = self.tavily_client.search(search_query)\n",
        "\n",
        "            # Save raw search results first\n",
        "            raw_results = \"# Raw Search Results\\n\\n\"\n",
        "            raw_results += f\"Search Query: {search_query}\\n\\n\"\n",
        "            raw_results += \"## Full Results:\\n\"\n",
        "            for idx, result in enumerate(search_results[\"results\"], 1):\n",
        "                raw_results += f\"\\n### Result {idx}\\n\"\n",
        "                raw_results += f\"Title: {result['title']}\\n\"\n",
        "                raw_results += f\"URL: {result['url']}\\n\"\n",
        "                raw_results += f\"Content:\\n{result['content']}\\n\"\n",
        "                raw_results += f\"Score: {result['score']}\\n\"\n",
        "                raw_results += \"-\" * 80 + \"\\n\"\n",
        "\n",
        "            save_to_markdown(\n",
        "                content=raw_results,\n",
        "                filename=\"raw_search_results\",\n",
        "                output_dir=\"article_outputs\"\n",
        "            )\n",
        "\n",
        "            # Process results for the writer\n",
        "            research_data = \"## Processed Research Findings\\n\\n\"\n",
        "            research_data += f\"Based on search query: {search_query}\\n\\n\"\n",
        "\n",
        "            for result in search_results[\"results\"][:3]:\n",
        "                research_data += f\"### {result['title']}\\n\"\n",
        "                research_data += f\"Source: {result['url']}\\n\"\n",
        "                research_data += f\"{result['content']}\\n\\n\"\n",
        "\n",
        "            # Save processed research findings\n",
        "            save_to_markdown(\n",
        "                content=research_data,\n",
        "                filename=\"processed_research_data\",\n",
        "                output_dir=\"article_outputs\"\n",
        "            )\n",
        "\n",
        "            return research_data\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error in web search: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            save_to_markdown(\n",
        "                content=error_msg,\n",
        "                filename=\"research_error_log\",\n",
        "                output_dir=\"article_outputs\"\n",
        "            )\n",
        "            return \"Research data unavailable.\"\n",
        "\n",
        "class WriterAgent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            role_instructions=\"\"\"You are an expert content writer. Your role is to:\n",
        "            1. Create engaging and informative content based on provided research\n",
        "            2. Maintain consistent tone and style\n",
        "            3. Include relevant examples and data points\n",
        "            4. Structure content with clear sections and transitions\n",
        "            5. Optimize for readability and engagement\"\"\"\n",
        "        )\n",
        "\n",
        "    async def write_article(self, article_idea: str, research_data: str) -> str:\n",
        "        prompt = f\"\"\"Please write a comprehensive article based on this topic and research:\n",
        "\n",
        "        ARTICLE TOPIC:\n",
        "        {article_idea}\n",
        "\n",
        "        RESEARCH DATA:\n",
        "        {research_data}\n",
        "\n",
        "        Write an engaging and informative article that:\n",
        "        1. Incorporates key insights from the research\n",
        "        2. Uses specific examples and data points\n",
        "        3. Has clear sections and structure\n",
        "        4. Maintains professional tone\n",
        "\n",
        "        Format your response as:\n",
        "        <ARTICLE>\n",
        "        Your article content here with proper sections and formatting\n",
        "        </ARTICLE>\"\"\"\n",
        "\n",
        "        response = await self.execute_turn(prompt)\n",
        "\n",
        "        # Save writer's output\n",
        "        save_to_markdown(\n",
        "            content=response,\n",
        "            filename=\"writer_draft\",\n",
        "            output_dir=\"article_outputs\"\n",
        "        )\n",
        "\n",
        "        return response\n",
        "\n",
        "class EditorAgent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            role_instructions=\"\"\"You are an expert content editor. Your role is to:\n",
        "            1. Review and improve article structure and flow\n",
        "            2. Enhance clarity and readability\n",
        "            3. Check for consistency in tone and style\n",
        "            4. Verify SEO optimization\n",
        "            5. Suggest specific improvements\n",
        "            6. Calculate readability and SEO scores\"\"\"\n",
        "        )\n",
        "\n",
        "    async def edit_article(self, article_content: str, original_requirements: str) -> str:\n",
        "        prompt = f\"\"\"Please edit and improve the following article:\n",
        "        Title: {original_requirements}\n",
        "        Content: {article_content}\n",
        "\n",
        "        Provide your response in the following format:\n",
        "\n",
        "        <EDITED_CONTENT>\n",
        "        Your edited article here...\n",
        "        </EDITED_CONTENT>\n",
        "\n",
        "        <IMPROVEMENTS>\n",
        "        - Improvement 1\n",
        "        - Improvement 2\n",
        "        - Improvement 3\n",
        "        </IMPROVEMENTS>\n",
        "\n",
        "        <SCORES>\n",
        "        SEO: <score 0-100>\n",
        "        Readability: <score 0-100>\n",
        "        </SCORES>\"\"\"\n",
        "\n",
        "        response = await self.execute_turn(prompt)\n",
        "\n",
        "        # Save editor's output\n",
        "        save_to_markdown(\n",
        "            content=response,\n",
        "            filename=\"editor_revision\",\n",
        "            output_dir=\"article_outputs\"\n",
        "        )\n",
        "\n",
        "        return response\n",
        "\n",
        "async def run_article_pipeline(topic: str = \"AI in Healthcare\"):\n",
        "    print(\"🚀 Starting Article Pipeline\")\n",
        "\n",
        "    try:\n",
        "        output_dir = \"article_outputs\"\n",
        "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Initialize agents\n",
        "        researcher = ResearchAgent()\n",
        "        writer = WriterAgent()\n",
        "        editor = EditorAgent()\n",
        "\n",
        "        print(\"\\n📝 Creating agents...\")\n",
        "        researcher.create_agent()\n",
        "        writer.create_agent()\n",
        "        editor.create_agent()\n",
        "\n",
        "        # Save initial article idea\n",
        "        article_idea = \"\"\"**Article Title:** \"The Future of Healthcare: How AI is Revolutionizing Patient Care\"\n",
        "        **Target Audience:** Healthcare professionals, patients, and tech-savvy individuals\n",
        "        **Key Points:**\n",
        "        1. Introduction to AI in healthcare\n",
        "        2. AI-assisted diagnosis\n",
        "        3. Personalized medicine\n",
        "        4. Predictive analytics\n",
        "        5. Ethical considerations\n",
        "        **Word Count:** 800-1000 words\"\"\"\n",
        "\n",
        "        save_to_markdown(\n",
        "            content=article_idea,\n",
        "            filename=\"initial_outline\",\n",
        "            output_dir=output_dir\n",
        "        )\n",
        "\n",
        "        print(\"\\n🔍 Conducting research...\")\n",
        "        research_data = await researcher.conduct_research(article_idea)\n",
        "\n",
        "        print(\"\\n✍️ Writing article...\")\n",
        "        article_draft = await writer.write_article(article_idea, research_data)\n",
        "\n",
        "        print(\"\\n📋 Editing article...\")\n",
        "        edited_result = await editor.edit_article(article_draft, article_idea)\n",
        "\n",
        "        # Save final report\n",
        "        final_report = f\"\"\"# Article Generation Report\n",
        "\n",
        "## Pipeline Stages:\n",
        "1. Initial Outline\n",
        "2. Research Findings\n",
        "3. Writer's Draft\n",
        "4. Editor's Revision\n",
        "\n",
        "## Files Generated:\n",
        "- Initial Outline\n",
        "- Research Data\n",
        "- Writer's Draft\n",
        "- Editor's Revision\n",
        "\n",
        "Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "        \"\"\"\n",
        "\n",
        "        save_to_markdown(\n",
        "            content=final_report,\n",
        "            filename=\"generation_report\",\n",
        "            output_dir=output_dir\n",
        "        )\n",
        "\n",
        "        return edited_result\n",
        "\n",
        "    except Exception as e:\n",
        "        error_log = f\"\"\"# Error Log\n",
        "Time: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "Error: {str(e)}\n",
        "Details: {traceback.format_exc()}\n",
        "        \"\"\"\n",
        "        save_to_markdown(\n",
        "            content=error_log,\n",
        "            filename=\"error_log\",\n",
        "            output_dir=output_dir\n",
        "        )\n",
        "        raise\n",
        "\n",
        "# Run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting pipeline execution...\")\n",
        "    asyncio.run(run_article_pipeline())"
      ]
    }
  ]
}
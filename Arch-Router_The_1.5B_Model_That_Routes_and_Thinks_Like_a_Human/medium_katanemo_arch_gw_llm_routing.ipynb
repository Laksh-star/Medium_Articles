{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio transformers torch\n",
        "!pip install gradio_consilium_roundtable"
      ],
      "metadata": {
        "id": "sF1QGtSxB8bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s6QHgiTwCH5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified from https://huggingface.co/spaces/tejasashinde/archRouter_simulator/blob/main/app.py\n",
        "# Integrates OpenAI API for real task execution instead of simulating completion\n",
        "import gradio as gr\n",
        "import time\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import json\n",
        "import ast\n",
        "import requests\n",
        "from openai import OpenAI\n",
        "import os\n",
        "# Removed import of gradio_consilium_roundtable\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"katanemo/Arch-Router-1.5B\"\n",
        "WAIT_DEPARTMENT = 5\n",
        "WAIT_SYSTEM = 5\n",
        "route_config = [\n",
        "    {\"name\": \"code_generation\", \"description\": \"Generating code based on prompts\"},\n",
        "    {\"name\": \"creative_writing\", \"description\": \"Creative writing or storytelling\"},\n",
        "    {\"name\": \"casual_conversation\", \"description\": \"General conversation or chit-chat\"},\n",
        "    {\"name\": \"math_reasoning\", \"description\": \"Mathematical problems or logical reasoning\"},\n",
        "    {\"name\": \"other\", \"description\": \"Any other request that does not fit a specific category\"}\n",
        "]\n",
        "departments = {\n",
        "    \"code_generation\": {\"name\": \"Code Generation\", \"emoji\": \"üíª\"},\n",
        "    \"creative_writing\": {\"name\": \"Creative Writing\", \"emoji\": \"üìù\"},\n",
        "    \"casual_conversation\": {\"name\": \"Casual Conversation\", \"emoji\": \"üí¨\"},\n",
        "    \"math_reasoning\": {\"name\": \"Math Reasoning\", \"emoji\": \"üßÆ\"},\n",
        "    \"other\": {\"name\": \"Other\", \"emoji\": \"‚ùì\"}\n",
        "}\n",
        "# Map routes to OpenAI models\n",
        "ROUTE_TO_MODEL = {\n",
        "    \"code_generation\": \"gpt-4o\",\n",
        "    \"creative_writing\": \"gpt-4o\",\n",
        "    \"casual_conversation\": \"gpt-4o-mini\",\n",
        "    \"math_reasoning\": \"gpt-4o\",\n",
        "    \"other\": \"gpt-4o-mini\"\n",
        "}\n",
        "\n",
        "# Load model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\n",
        "model.to(device)\n",
        "\n",
        "def format_prompt(conversation, route_config):\n",
        "    route_descriptions = \"\\n\".join([f\"<route name=\\\"{r['name']}\\\">{r['description']}</route>\" for r in route_config])\n",
        "    messages = []\n",
        "    for msg in conversation:\n",
        "        role = msg[\"role\"]\n",
        "        content = msg[\"content\"]\n",
        "        messages.append(f\"**{role.capitalize()}**: {content}\")\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id>\n",
        "\n",
        "You are a router model. Analyze the conversation below and select the best route from the list. Output only a JSON object like {{\"route\": \"route_name\"}}. If no route matches, use \"other\".\n",
        "\n",
        "Routes:\n",
        "{route_descriptions}\n",
        "\n",
        "Conversation:\n",
        "{'\\n'.join(messages)}<|eot_id|><|start_header_id|>user<|end_header_id>\n",
        "\"\"\"\n",
        "\n",
        "def parse_route(output):\n",
        "    try:\n",
        "        start = output.find(\"{\")\n",
        "        end = output.rfind(\"}\") + 1\n",
        "        json_str = output[start:end]\n",
        "        return ast.literal_eval(json_str)[\"route\"]\n",
        "    except:\n",
        "        return \"other\"\n",
        "\n",
        "# init_state and visualization logic removed\n",
        "# Replacing with a simple processing function\n",
        "def process_query(input_text):\n",
        "    if not input_text:\n",
        "        return \"Please enter a query.\"\n",
        "\n",
        "    print(\"üîé Identifying route, please wait...\")\n",
        "\n",
        "    conversation = [{\"role\": \"user\", \"content\": input_text}]\n",
        "    prompt = format_prompt(conversation, route_config)\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        conversation=[{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": input_text}],\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "    outputs = model.generate(inputs, max_new_tokens=512, do_sample=False)\n",
        "    raw_output = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
        "    print(f\"Raw model output: {raw_output}\")\n",
        "    route = parse_route(raw_output)\n",
        "\n",
        "    dept_name = departments.get(route, departments[\"other\"])[\"name\"]\n",
        "    dept_emoji = departments.get(route, departments[\"other\"])[\"emoji\"]\n",
        "    print(f\"üìå Identified department: **{dept_name}**. Forwarding task...\")\n",
        "\n",
        "    print(f\"{dept_emoji} {dept_name} is processing your request...\")\n",
        "\n",
        "    # Call OpenAI API\n",
        "    try:\n",
        "        # Retrieve API key from Colab Secrets Manager\n",
        "        openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "        client = OpenAI(api_key=openai_api_key)\n",
        "        response = client.chat.completions.create(\n",
        "            model=ROUTE_TO_MODEL.get(route, ROUTE_TO_MODEL[\"other\"]),\n",
        "            messages=[{\"role\": \"user\", \"content\": input_text}],\n",
        "            max_tokens=1000,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        llm_output = response.choices[0].message.content\n",
        "        print(f\"‚úÖ {dept_name} completed the task:\\n\\n{llm_output}\")\n",
        "        return llm_output\n",
        "    except Exception as e:\n",
        "        error_msg = f\"‚ùå Error from {dept_name}: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return error_msg\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks(theme=gr.themes.Ocean()) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # Arch Router Simulation: Smart Department Dispatcher\n",
        "    This demo simulates an AI router using the [katanemo/Arch-Router-1.5B](https://huggingface.co/katanemo/Arch-Router-1.5B) model.\n",
        "    Enter a query to see it routed to the appropriate department (e.g., Code Generation, Creative Writing),\n",
        "    which then generates a response using an OpenAI model.\n",
        "    The routing process and the final response will be printed below.\n",
        "    Try prompts like:\n",
        "    - \"Write Python code for a calculator that handles addition, subtraction, multiplication, and division with error handling.\"\n",
        "    - \"Write a short poem about the ocean.\"\n",
        "    - \"What's the weather like today?\"\n",
        "    \"\"\")\n",
        "\n",
        "    input_text = gr.Textbox(label=\"Your Query\", placeholder=\"Enter your query here...\")\n",
        "    output_text = gr.Textbox(label=\"LLM Output\", interactive=False)\n",
        "    submit_btn = gr.Button(\"Submit\")\n",
        "    example_prompts = gr.Examples(\n",
        "        examples=[\n",
        "            \"Write Python code for a calculator that handles addition, subtraction, multiplication, and division with error handling.\",\n",
        "            \"Write a short poem about the ocean.\",\n",
        "            \"What's the weather like today?\",\n",
        "            \"Solve the equation 2x + 5 = 15.\"\n",
        "        ],\n",
        "        inputs=[input_text]\n",
        "    )\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=process_query,\n",
        "        inputs=[input_text],\n",
        "        outputs=[output_text]\n",
        "    )\n",
        "    input_text.submit(\n",
        "        fn=process_query,\n",
        "        inputs=[input_text],\n",
        "        outputs=[output_text]\n",
        "    )\n",
        "\n",
        "demo.launch(debug=\"true\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "BrfIQNjrT3z6",
        "outputId": "3794d622-732e-48c0-a418-c0939b981dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://27b85daa1d93ff7771.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://27b85daa1d93ff7771.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Identifying route, please wait...\n",
            "Raw model output: {'route': 'creative_writing'}\n",
            "üìå Identified department: **Creative Writing**. Forwarding task...\n",
            "üìù Creative Writing is processing your request...\n",
            "‚ùå Error from Creative Writing: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7864 <> https://27b85daa1d93ff7771.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}
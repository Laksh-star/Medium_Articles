{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ISaajxYqdW8"
   },
   "source": [
    "**1: Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJvr5BF0qX2a"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37lEg9SrqkAe"
   },
   "source": [
    "**2: Load Base Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494,
     "referenced_widgets": [
      "9e18bc1fae8d4e1284ce5b18d8193b42",
      "90768117af964d1f990737a0dbd8659d",
      "59a33984a9364b2eabfc12a74faa667d",
      "d21def4ac0c84f4bb305f0f72370cd3d",
      "605578b7d6764af9b34d8cd0772a312d",
      "5da50fb90b234885861792e62ea7121e",
      "cd6319a6e6df4673aa784abd79e3abd8",
      "33584a81f499433a8c31d4024e396768",
      "f1ea7072f29e4ac88e3f6107a928b38e",
      "b8c2ed3601d641bdb0fe3dff9ef0acf0",
      "9ce5fa8f0ff6448da5fa83087dc0b193",
      "bfdbcbf2a5a044e9b8206568b61a4136",
      "08b95e43ccc442c182b0f37c93dae755",
      "84d987596cb54181a2a817f1c06c0951",
      "3c0afa6fe6eb4714a07a1485e8288ef4",
      "db857d9e732847519fb5901742d6bcf5",
      "065d88ef6239493186451c84937aba98",
      "d6e2af3097a44308af5835abb6114cfb",
      "35e948f629da4f85a4062fdaa9e72bca",
      "5b37ffd4363d4561ae7e01ca4208fc70",
      "bbbc5121ee8a45b5b0b86fb0348f54b3",
      "e911556db6ec4549a2560afea5c67925",
      "1fd68c788f934872a04b4239d5cb9dba",
      "7200497d8efb4c9e9b982ab7f05537c9",
      "3a90f04d5b41493c9f3fa06cf169ac9f",
      "8c6eba6488c84a90834786cee25470a7",
      "14cef3308cfa4ff4967763d0a885120e",
      "ea27eda552a344e686eb9cded878763f",
      "03ecc0cdcbfa48e2851bd827e6075e99",
      "a1f41814b8b144dba41f067a96c38b5c",
      "ef72156fe3024ac2b3530ff5b3fe9bc3",
      "4e20d8df1120499a9abbd63627fda916",
      "516c77aeb4ab48368b6d98fbd247ce1a",
      "48619364d846456d92d1445e5ed577cc",
      "a3ec47346b634caeab726ef3a5862f79",
      "2830af3287d64f12b4d0f847388bb67a",
      "49c401b71c614a0685d50f59cd8840f5",
      "11a5747f5fa84347ab6d827dde48d2d3",
      "a41e8c5fc6d4463f85f04898ab2574d6",
      "5b06951265c142fa8935bbf46ca80429",
      "6adaa45309bd40c3a25f0c5020812d48",
      "f35fffdcecf8472e80290347afed3501",
      "211c37fd08154b8284e7eea02f190dc1",
      "544acb55d2664495b12f84a71b4dc554",
      "25f1f8e2cee04cf48d0956fd1a3aafa9",
      "cd66c06c81284a32b085d0d6d4a85d9f",
      "1f0528d5767c493b9669de61a69b231d",
      "46b5f961a4454cd0970154b4986951cb",
      "0232ad0eb63541248c556b23bbfe641a",
      "02792daf08f940148cbf1c9e8e022b10",
      "722ad69b73eb4ed9b39874ccc9cdf788",
      "93ec17714ac04f229316fe62302e32a5",
      "b26ca001be734255947564180b03c823",
      "72a8fd5491a44180b436ff9d1710b121",
      "1f44eb38753e47ac891a0432916dca11",
      "d118508e9bc34b748142ecfc113446be",
      "8f0fd401800a4949aabc378780acb79e",
      "f49f77071d2b49cb8c14337004632c74",
      "4f93e1e5ab564cd0a586d9df6b346346",
      "6ddc56022cd546aeb6fc5d4f9943cadd",
      "ab8402cfb0024c94bf8813cae15bc810",
      "43608c9e066b4fb89a603ac20d1dac0b",
      "0f7b8ddf6d2644c9a03ee4060a753ae4",
      "dafb054f3a7a49bdabb94051de6ee2e9",
      "6b64393b28ad434092e67fe341b77c2b",
      "aa4a20ac80df49d580733277f4ebce1c",
      "93e16ed9e0f0420d95d9478f03458dc9",
      "de404bdbca024aeaa4e9fa990747bf3b",
      "9347e78d1285419aa8a038694e3bade5",
      "e2b943e097b342ada067a8380486273c",
      "70c855e58805498cb386e9876beaeee0",
      "fa2fd926bb5b4c4e91aac9a7d9809aa7",
      "d1993ff4eb85404d94c59128b94fdde4",
      "57d5a9c3c131476eabfbfc6184f35c6b",
      "40c715d99c2a4a3ca8001b6687efc31d",
      "ca6e5fbf6115436e8e15839edfd18f87",
      "373f3cd0edb64e449326c6e26cc25748",
      "c0f7a23eb3594fb7a038ab3021c6b24f",
      "2a6eb0686eb64473802d7a34e7fa4778",
      "b860e13b1d614e0c9c73634841e5e6e4",
      "8f80aff53c6741bd89d600460e82aa09",
      "8f95da07ce794d9abb7800e534cdb331",
      "99716dc31e404f8886ee180e3ec2db53",
      "467a9aa93b9848c086d84f9c492c7f0d",
      "a10ddbe8b63041d8b08f61eda2f29113",
      "708fdc0260c14cbaa2b3c17273623fa5",
      "4fdfd8f927264129abcaede14d3b3b14",
      "9a0c7b11f9dd45f0bad4dc9417046b73"
     ]
    },
    "id": "D7THd7Gdqp_W",
    "outputId": "8225e7d3-f4b0-4b80-ea96-e72b418aef07"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-270m-it\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit = False,\n",
    "    full_finetuning = False,\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"Model: {model.config.name_or_path}\")\n",
    "print(f\"Max sequence length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0A9nDx5UqtPX"
   },
   "source": [
    "**3: Add LoRA Adapters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwQjHiWJq0w7",
    "outputId": "fa3efbdc-3038-4838-c61f-71c55e37e48c"
   },
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Higher rank for medical domain\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 128,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA adapters added successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61T7ZIfQq2rj"
   },
   "source": [
    "**4: Setup Chat Template**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aWE7O7Ddq720",
    "outputId": "7ef0b87d-d842-44bb-bcc6-fa5c49c7b1f8"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma3\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Chat template configured for Gemma-3!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwCAS5i3rAuz"
   },
   "source": [
    "**5: Load and Inspect MedMCQA Dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618,
     "referenced_widgets": [
      "06f82ad7ea374f56a7f3b833e1dbb4e8",
      "06259396cd7e48afb37c394f8cb7e62f",
      "40354138426b4101aba5378e45ccff51",
      "5e1589d865b94bb9a184227d11824e32",
      "8d972ad8b2fe4a2d9989311eb3294e8c",
      "716f78457d344f01bda74183fbd264aa",
      "809dbfcecb554127bb6e3fd1842db1fc",
      "e3909cfdaf884376b1a76029ef96ceec",
      "aa88089077744bdaa02fe9e92702e715",
      "6434fde9ee4e479cb3746c6db1bae6a5",
      "6dbe4b9e2d5946a2ae0886190c70ec09",
      "e9478e9b3e674bce9d1e63b0a3405761",
      "a8287184f94c49fb9c393f6b5801d86d",
      "4d21942cbd794434b032cdd8a48cfcd1",
      "9e363226262a45b89f1c262b965740a1",
      "89d67e9216d841c1bf26214f33e7c268",
      "fad1f739c1c54560a394b0e6b85ee42f",
      "671c5081287a43edbed89db44e98c7c1",
      "81d56a0bb1c04d59be6385740e19ca68",
      "f58676df343e431fbed5c9f1c66d75f2",
      "07e7c9d6a8eb4d63ad104e9478ed2fb9",
      "f2529c5f036e494480828aeed0a929d9",
      "fc597bfd1d2544529974333bc7080cc3",
      "d28b8fa74399441ca24893b823368fd9",
      "f9c2bc29f7e74c8b9e39fc3a106625d7",
      "308d96e58817442385882d0db11b0121",
      "7225ea1fb7484a84a65d0f7963288955",
      "3ece90127f0d479d85a7b97a7bf2fb2f",
      "7f4a479abbf0477f9697091cf4ffc004",
      "65f0e94d14224144ad581836df85ea29",
      "4ab9a80ee5454d4bb400ca254000f7b4",
      "202cfd4e32644d13adf5c9b9f0a46728",
      "e152da73bd9b4668b410674016525499",
      "22c45b6e14f94b099c347e15908f902e",
      "6331ea75e71c4c7f9e86825809367412",
      "12fdeb329e6f4ed1ad69942db20e33d0",
      "a48f091fad6f4f9db9f8be52c886d352",
      "379117aecd614861bded293d04139573",
      "576ee266a84445caa93f369c84bab58c",
      "cb0230a0b5224960a3e176094a542965",
      "91be849d9f9046aea30c3c5141137bd6",
      "ad1381aae8f64aab9992e6807368059d",
      "7e3f33edff6a41f5b2b3b41ef40e0fbe",
      "065540a722244bdd8fdf161fce23efd1",
      "9c8bcf5ecf8c4ebba5ae017191237922",
      "8108c6fa7eed4b3e93482a90a897266a",
      "b34efb412cc145dd9aa0c5b50def6f39",
      "b65062707e6541cb972c5c1f7a59c2c0",
      "a957a663b07a47cbace60837dade06bd",
      "d8edb4bf74da4e4197dd024468eafc24",
      "8a30f714f1884a129c94373865be3668",
      "16ecc8e574e94b7989f4e7fe12d62c45",
      "42697ab257664689a5dd66d372994e42",
      "954fb8336cff43a5b056e73efab78295",
      "3c0d085ff3cf4c2f9516fd061109a5a2",
      "8cca79bed229433594617c1c2d92a264",
      "72fa0755d03844cca5d6bdb98a851d57",
      "83f39a3e34804ca1b9156269252f86e8",
      "3bc2332389fc4b98b35f52ebd2f1519f",
      "2313904dd59342a9bcd0ec631d5d04cc",
      "518705f7bb4d45ee8f558da25fd17f4c",
      "ea4cd13328444d7398c99be2790b0d7d",
      "176f1344c5b4456e8e6484baaa0afd13",
      "47a9f890eb504fe7b5f4771a427330c1",
      "17a6030ae56849a8bfb67af591fc2818",
      "558c05e2dca74bf98f1a510d8eb80e39",
      "d2fb460bba13442086b3c27ef35d6a79",
      "d43ac73e171f45aa9d4571bf4ee17b6b",
      "48f536a73bab41e093450e5a987dc2e6",
      "2d9ea3c9398f4897ad5d368825c0e0e0",
      "dbde5a52d16945c2824cec79135606b3",
      "7f41c217374240d6b355c9161706bed9",
      "f59ba654cef0407581b628c974ed1eba",
      "593a98ada45f43369fb82cbae36163be",
      "59bc589218c442b5baf38e277aba0ba5",
      "14f87a91fa384083a1bb86c93d678cb7",
      "8c1605cab0464cc288af05719e2dfec3"
     ]
    },
    "id": "jl7F0_joq-AU",
    "outputId": "90e4e1b3-940b-4708-e2d9-587669dcc098"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the MedMCQA dataset\n",
    "print(\"Loading MedMCQA dataset...\")\n",
    "dataset = load_dataset(\"medmcqa\", split = \"train[:15000]\")  # Start with 15k for training\n",
    "\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE QUESTION:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show sample question\n",
    "sample = dataset[0]\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Options:\")\n",
    "for i, option in enumerate(['opa', 'opb', 'opc', 'opd']):\n",
    "    if sample[option]:  # Only show non-empty options\n",
    "        print(f\"  {chr(65+i)}) {sample[option]}\")\n",
    "print(f\"Correct Answer: {sample['cop']} ({chr(65+sample['cop']-1)})\")\n",
    "print(f\"Subject: {sample.get('subject_name', 'N/A')}\")\n",
    "if sample.get('exp'):\n",
    "    print(f\"Explanation: {sample['exp'][:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET STRUCTURE:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Dataset features:\", dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sgvj9orKrMwk"
   },
   "source": [
    "**6: Convert Dataset to Medical Chat Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392,
     "referenced_widgets": [
      "4d6c449278864d619f4bf2028932d3a6",
      "4cbcbdefae80449ebb7429bb87dc3829",
      "af35f92fdf1f4644b32df36ab031d419",
      "3765026b540640edae02fbac2d15aca0",
      "663cb183b9d04f818056db91a42a2c65",
      "0e43ad61915f42a49f20e9990c8d8efc",
      "17c3bfd20a6e4727b58d7a8ac4d1eb53",
      "4dbca8ddae0f410f8bfa492aaabeff48",
      "c767b7e77b5e4b7bb393c55424affb0a",
      "5dee950640b8445e8dc4f08b4ccc3b72",
      "3004e5be605d490eb3d9a87298f1b73d"
     ]
    },
    "id": "WVAmq4korI4s",
    "outputId": "63695904-2497-4749-d166-abf4b95cd8c3"
   },
   "outputs": [],
   "source": [
    "def convert_medmcqa_to_chat(example):\n",
    "    \"\"\"\n",
    "    Convert MedMCQA format to medical assistant chat format\n",
    "    \"\"\"\n",
    "    question = example['question']\n",
    "\n",
    "    # Build options text\n",
    "    options = []\n",
    "    option_letters = ['A', 'B', 'C', 'D']\n",
    "    for i, option_key in enumerate(['opa', 'opb', 'opc', 'opd']):\n",
    "        if example[option_key]:  # Only include non-empty options\n",
    "            options.append(f\"{option_letters[i]}) {example[option_key]}\")\n",
    "\n",
    "    options_text = \"\\n\".join(options)\n",
    "\n",
    "    # Create the question with options\n",
    "    full_question = f\"{question}\\n\\nOptions:\\n{options_text}\"\n",
    "\n",
    "    # Get correct answer\n",
    "    correct_option = example['cop']  # 1, 2, 3, or 4\n",
    "    correct_letter = option_letters[correct_option - 1]  # Convert to A, B, C, D\n",
    "    correct_text = example[f\"op{'abcd'[correct_option-1]}\"]\n",
    "\n",
    "    # Create answer with explanation if available\n",
    "    answer = f\"The correct answer is {correct_letter}) {correct_text}\"\n",
    "    if example.get('exp') and example['exp'].strip():\n",
    "        answer += f\"\\n\\nExplanation: {example['exp']}\"\n",
    "\n",
    "    return {\n",
    "        \"conversations\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a knowledgeable medical assistant. Provide accurate medical information based on established medical knowledge. Always recommend consulting healthcare professionals for medical decisions.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": full_question\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": answer\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Apply conversion\n",
    "print(\"Converting dataset to chat format...\")\n",
    "dataset = dataset.map(convert_medmcqa_to_chat)\n",
    "print(\"‚úÖ Dataset converted successfully!\")\n",
    "\n",
    "# Show converted example\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONVERTED EXAMPLE:\")\n",
    "print(\"=\"*60)\n",
    "conv = dataset[0][\"conversations\"]\n",
    "print(\"System:\", conv[0][\"content\"])\n",
    "print(\"\\nUser:\", conv[1][\"content\"][:300] + \"...\")\n",
    "print(\"\\nAssistant:\", conv[2][\"content\"][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKEnLR3vrWfd"
   },
   "source": [
    "**7: Apply Chat Template to Dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409,
     "referenced_widgets": [
      "29264345307f47438f4e098dd38b6db4",
      "a4ec353b600846fba316b25f706a0a3f",
      "2af29e2b9bf14003b503f3e818e30fbe",
      "b6b988f28bf84db9b79d55bb803aa813",
      "b9dc9c0d2b9b46e08f4ce9b6e453caa4",
      "65c7faca2aa54baa82edc0ec2b06dcdf",
      "5941241b706845de89ba82dc395f900b",
      "0a47a2ba834047acb33a04f09f8c8f85",
      "91a0f33ede154ef3aef629cba11d5b4f",
      "eabc184feaee44ffb4099b812b567884",
      "ecb52fe4fd104bbe953df5f8c9913dfc"
     ]
    },
    "id": "UjJPzrtjrSAR",
    "outputId": "d6572f4c-8b71-48f0-999c-477a87df97c7"
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "   convos = examples[\"conversations\"]\n",
    "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
    "   return { \"text\" : texts, }\n",
    "\n",
    "print(\"Applying chat template...\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "\n",
    "print(\"‚úÖ Chat template applied!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FORMATTED TRAINING EXAMPLE:\")\n",
    "print(\"=\"*60)\n",
    "print(dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tkf-UnJOraQ1"
   },
   "source": [
    "**8: Setup Training Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134,
     "referenced_widgets": [
      "bb343055bc8545d5a63dec11a742c98c",
      "debb433f62934b30b64c4b9e29b0d929",
      "b25d5dbb70754249b42fd8c315d15c7a",
      "9afdbce0a6b449bca2bb0b0f6048134a",
      "3abb83c2950240fa88c75a133978cf2d",
      "4d56f1a875f14cab84589d804ab388d8",
      "95f0019f7f3f4925a486143ed16faf4b",
      "e9f58a0d312e458fbc88bf650305674f",
      "07121bb5085f48b28237f02289c37b7e",
      "ae49bbedbe2646e180c789173290357f",
      "a065d0152c234fd2ab3fbd6ec19dfe21"
     ]
    },
    "id": "QTfGE-YGrdZa",
    "outputId": "c03b3a68-a95a-4b4d-eeb7-4de0864cc8c9"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 50,\n",
    "        max_steps = 1000,  # Good for 15k medical questions\n",
    "        learning_rate = 3e-5,  # Slightly lower for medical domain\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir=\"medical_qa_outputs\",\n",
    "        report_to = \"none\",\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 500,\n",
    "        eval_strategy = \"no\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration set up!\")\n",
    "print(f\"Training on {len(dataset)} medical questions\")\n",
    "print(f\"Max steps: 1000\")\n",
    "print(f\"Learning rate: 3e-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z--GZbI9rkZy"
   },
   "source": [
    "**9: Configure Medical Response Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "9eb58e9be7b74a498ab9be3b27c959da",
      "103b6a55878c459192e10ad153969d60",
      "6871f374eaf946289a26235da1e1f3c7",
      "d6da9ebefd204a9cbbfc54a266ed09e6",
      "c3ebc95f2b3b4cc69f5230d5908bf37c",
      "90ea43e58d454859b0c0cb436b149727",
      "093ae9b9c2084ec6b3d361a93749818c",
      "7af5524ab8934a32874329ac51ce6105",
      "0c27986dddc84bcfabd82c88023cefe3",
      "9a9991167cde46518fde687e4fd5215e",
      "2b0e4b65656f44469fec409e56fdb5d3"
     ]
    },
    "id": "W8K_7tpgrpXv",
    "outputId": "7163688e-dda2-4065-b422-f5858a45efeb"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Configured to train only on medical responses!\")\n",
    "print(\"This ensures the model learns medical knowledge without overfitting to questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OV5mSpEFrvF5"
   },
   "source": [
    "**10: Check Memory Usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2ZwMnnhrumN",
    "outputId": "a2dedade-b823-4b6d-f441-6e10815e83ee"
   },
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(\"üíª SYSTEM INFORMATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"GPU: {gpu_stats.name}\")\n",
    "print(f\"Max memory: {max_memory} GB\")\n",
    "print(f\"Memory reserved: {start_gpu_memory} GB\")\n",
    "print(f\"Available memory: {max_memory - start_gpu_memory} GB\")\n",
    "\n",
    "if max_memory > 10:\n",
    "    print(\"‚úÖ Sufficient memory for training!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Limited memory - consider reducing batch size if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEOzdJuGr3pz"
   },
   "source": [
    "**11: Start Medical Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "t0zG3sJOr74I",
    "outputId": "b6e468c0-c270-416d-aa08-db3450f8937a"
   },
   "outputs": [],
   "source": [
    "print(\"üè• STARTING MEDICAL Q&A TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(\"Training the model on medical knowledge...\")\n",
    "print(\"This will take approximately 20-30 minutes on free Colab.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ TRAINING COMPLETED!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sbbu4hSmr_Wm"
   },
   "source": [
    "**12: Training Statistics**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "IdophIvusCb5",
    "outputId": "693edb76-0fc7-4566-819e-cab1473dfce6"
   },
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(\"üìä TRAINING STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚è±Ô∏è  Training time: {round(trainer_stats.metrics['train_runtime']/60, 2)} minutes\")\n",
    "print(f\"üíæ Peak memory usage: {used_memory} GB ({used_percentage}%)\")\n",
    "print(f\"üìà Memory for training: {used_memory_for_lora} GB ({lora_percentage}%)\")\n",
    "print(f\"üéØ Final training loss: {trainer_stats.log_history[-1].get('train_loss', 'N/A')}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHXmDBS-sOjc"
   },
   "source": [
    "**13: Test Medical Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed3LfZJBsRT-"
   },
   "outputs": [],
   "source": [
    "print(\"üß™ TESTING MEDICAL Q&A CAPABILITIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Medical test questions\n",
    "medical_test_questions = [\n",
    "    \"\"\"Which of the following is the most common cause of acute myocardial infarction?\n",
    "\n",
    "Options:\n",
    "A) Coronary artery spasm\n",
    "B) Atherosclerotic plaque rupture\n",
    "C) Coronary embolism\n",
    "D) Aortic stenosis\"\"\",\n",
    "\n",
    "    \"\"\"A 45-year-old patient presents with sudden onset chest pain radiating to left arm. What is the most appropriate initial investigation?\n",
    "\n",
    "Options:\n",
    "A) Chest X-ray\n",
    "B) Echocardiography\n",
    "C) 12-lead ECG\n",
    "D) Cardiac enzymes\"\"\",\n",
    "\n",
    "    \"\"\"Which drug is considered first-line treatment for type 2 diabetes mellitus?\n",
    "\n",
    "Options:\n",
    "A) Insulin\n",
    "B) Metformin\n",
    "C) Sulfonylureas\n",
    "D) Glitazones\"\"\",\n",
    "\n",
    "    \"\"\"What is the normal range for systolic blood pressure in adults?\n",
    "\n",
    "Options:\n",
    "A) 90-120 mmHg\n",
    "B) 120-140 mmHg\n",
    "C) 140-160 mmHg\n",
    "D) 160-180 mmHg\"\"\",\n",
    "]\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "for i, question in enumerate(medical_test_questions, 1):\n",
    "    print(f\"\\nüè• MEDICAL QUESTION {i}:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(question)\n",
    "    print(\"\\nü§ñ MEDICAL AI RESPONSE:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a knowledgeable medical assistant. Provide accurate medical information based on established medical knowledge. Always recommend consulting healthcare professionals for medical decisions.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = True,\n",
    "    ).removeprefix('<bos>')\n",
    "\n",
    "    _ = model.generate(\n",
    "        **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "        max_new_tokens = 200,\n",
    "        temperature = 0.3,  # Lower temperature for medical accuracy\n",
    "        top_p = 0.9,\n",
    "        do_sample = True,\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    "    )\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ Medical inference testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMtXXq4dsLdy"
   },
   "source": [
    "**14: Save Medical Model Locally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ndn2Rc7PsZZJ",
    "outputId": "8b5e956d-e10a-4521-c0c2-24fde1a756fc"
   },
   "outputs": [],
   "source": [
    "print(\"üíæ SAVING MEDICAL Q&A MODEL...\")\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"medical_qa_lora\")\n",
    "tokenizer.save_pretrained(\"medical_qa_lora\")\n",
    "\n",
    "print(\"‚úÖ Medical Q&A model saved locally!\")\n",
    "print(\"üìÅ Saved to: medical_qa_lora/\")\n",
    "print(\"\\nModel artifacts:\")\n",
    "print(\"- LoRA adapters (adapter_model.safetensors)\")\n",
    "print(\"- Model configuration (adapter_config.json)\")\n",
    "print(\"- Tokenizer files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0p8vAP5PsisN"
   },
   "source": [
    "**15: Push to Hugging Face (Replace YOUR_HF_TOKEN_HERE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341,
     "referenced_widgets": [
      "057874092d044eaa8c0297a7b032ef71",
      "5e548a83197d45aeb0ec97fe86bd0380",
      "5c26fbcebca4475d9dc90deb5edd048c",
      "c7fa48fbb5cf49e99a87fc78f530ed0f",
      "646bce0025db4f2d87a2befba2fb2eff",
      "3c7d42b1e4ae40548b75ae280b448ea3",
      "56bebb2b4ea14135a26e04c8ccb1d234",
      "0f8a139498e04893aaa1260b96e423c5",
      "5e00c61e42694411ade072c582a7ea00",
      "34c0d3314dba4e85ae45d3c400e179ca",
      "fb0847dad85c4758ba66035ffc1f0b68",
      "81edd396d25c4da4a776ec3d7d56f84b",
      "8544a88f7bc44f0f8414a0fddc665a52",
      "d8de59f4efb9493397fe8d51eb958107",
      "086a890515a441cbb69b7e9be8abaef1",
      "2a4c618d8d314acd835e2359b658edef",
      "1d6ff92a9e0241dfa8e40a144b0e5185",
      "8df2490915b94fbf88b76625799f8b27",
      "960a114fc42e4fcb9184de350a6f930e",
      "a776e6410d2541d1b96c0ec679052252",
      "eb84f055aac94cda8781407f0deb8834",
      "ea0c4e9f5c014038a78882fbe2ed719b",
      "0b6164e9faa24f5e921daefd7b82b66c",
      "61638d2f2ff44e6e9bae7f5221ea5519",
      "9e5d183325fc4baaba25d7bd30b87b96",
      "746ac55c625e4d179eca049877cb3e52",
      "1489a2ba63b4481a9c307e3a0fbbe3c6",
      "dbd31b5fb33845f7868d85762630c11a",
      "e38a7a13d4be40e6ac0c42bab308739b",
      "87eecfa01566403eb8a947de40e18d44",
      "25e3c82a98d94169b65e135f1732c58e",
      "00de98ed8a024e4789445ffbc0cd85f5",
      "2c6a08db18e64cdf9bd20735951122b1",
      "8d465df26be84a33b1adcdc978986ff4",
      "067f17e5952d40178a6ae948ec623dd4",
      "c6e13f8e294a40bd9953cd5bce792c9b",
      "45a2b2f3e250425b8ca9c3b29c0779df",
      "a856663382ad44d09c085b4be6498344",
      "87fd907096974615b52ebd647a4a6062",
      "4b044d3fa74c480b97a138ce002ea022",
      "0e2993e637b34922a617d7b21ec0d827",
      "f0073410e1e2425cafd88c3dbd77b7f8",
      "081a7e29f02a440d9e441385a7f1d404",
      "8a7741b816cc4b009f35e76c909505a6",
      "b41376f801454ec79191baee4e629fb5",
      "65c49a5a66c0489e910f894515b4c951",
      "9633cf8456754db487668e693c0b46fe",
      "20045a4028e44d549d44c9c9f7b36f77",
      "6445df4da2354d39970d00944dc5595b",
      "f6d4372a7aeb48a280a975df8ada6022",
      "90537bd1d9a54d6a916f234ab230c8e0",
      "d9a8921308ec451bac351a3dee584997",
      "21372090982f415885e26e5bb59c8b6f",
      "feacbf7fbc924158ad557f0c56887b60",
      "89d510b52b594a5abd884c0c5ff26153",
      "dc5af753fd4047e4b78996479013d38e",
      "9baad9f42fba4628a72dc2406ecfda4a",
      "5241aa2e553845f9b01e45320bfa6fc2",
      "d6e8e9eb86d348cf9038b34f1ca8444a",
      "0c536067be5f436bb5500f90e4b548fc",
      "797ec2ab1a0a4020a80bfff16e670e3a",
      "282e47fb4b1342269bc36ddab3451cfe",
      "c1256eaf13284424b3b33118d79e25c9",
      "70d70235b76a4784afe4aab48d246a9d",
      "97531cac8e91469188c3b4897706c5e0",
      "13fa0d8bea564884af0726083229886d",
      "92f5015cc5f7418fb7d762736b2ada68",
      "0cd6658768354d908ccffdcce9f2e5be",
      "011df398631e448fa976b8b9ae511446",
      "d365308464b84e6b91b83229d83e69e3",
      "8effbdfb5c15415dbaaf1f7c77cc521d",
      "42233e5737f8446eba566fd028c8c1d3",
      "f0f58fcb611c47ddadf4e1eafad532c3",
      "2012fd31608b403a88f7c573d5a5bc99",
      "77902e4aff674e56bf9b998573543e28",
      "7be9f64b759849fd8ca0117150035f9c",
      "5bfa77c71d4c49d19e9daca16a60b245",
      "efd8d3bd0f5f4b5d822f075545649b67",
      "79e6cf4f27f14a418c4a86a1a078ca4c",
      "c41d0e91554048388f1b51a764d802e8",
      "42adc8c5f8ff402aae53aca0325d75db",
      "c53de8e3f1f249e0aa5e45817fe2d24c",
      "b1e2407d20d04f879b8468958a572404",
      "775ad34e23b5466788cda798920d74aa",
      "b503190b5fa8426bb7283ba521590a4a",
      "07ce8c99cf944301a7d184735680775c",
      "0ede0f080a564672b1d76512e54f374b",
      "e0a4b3d00b2542cfac3f93ac424c992d"
     ]
    },
    "id": "HjUTYfACsme6",
    "outputId": "3242cd12-16c6-4303-d7a9-f8e886a2aa66"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: Replace YOUR_HF_TOKEN_HERE with your actual Hugging Face token\n",
    "\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"  # üîë Replace with your token!\n",
    "\n",
    "if HF_TOKEN != \"YOUR_HF_TOKEN_HERE\":\n",
    "    print(\"üöÄ PUSHING MEDICAL MODEL TO HUGGING FACE...\")\n",
    "\n",
    "    # Push LoRA adapters\n",
    "    model.push_to_hub(\"Laksh99/gemma-3-270m-medical-qa-lora\", token = HF_TOKEN)\n",
    "    tokenizer.push_to_hub(\"Laksh99/gemma-3-270m-medical-qa-lora\", token = HF_TOKEN)\n",
    "\n",
    "    print(\"‚úÖ LoRA adapters pushed successfully!\")\n",
    "    print(\"üîó Available at: https://huggingface.co/Laksh99/gemma-3-270m-medical-qa-lora\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please set your Hugging Face token to push the model!\")\n",
    "    print(\"Get your token from: https://huggingface.co/settings/tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60JT_rvVsps7"
   },
   "source": [
    "**16: Save and Push Merged Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462,
     "referenced_widgets": [
      "7071b35f10944a37be05a93146b1f4ec",
      "8b0d571998984f6d9b16ffaad83adda8",
      "f303c4907bdb4b8aab95b66f0f79488d",
      "aaa60a5f3b6e43eaab828df43fffba56",
      "b11347eaca814e4783337deb76b9b7db",
      "c2f640f9fe2c408ebdd100b3b3e9fbd9",
      "1dd5ae8e470e40ecbccd7065d43be914",
      "61ec56f2efa3480ead1a8297e88805e9",
      "9bb682b84c4a47f689e576a0f9174def",
      "6d17ca10b21a456f96bc5186119134de",
      "516d429acadf4c4484e80626d22f82ec",
      "c235809347194ac6aac6e6fde92dc928",
      "79e56838c8524b6c8e5f1a962515f194",
      "d48286a2beb744e3b015676b5e6f1e94",
      "9ef4dad77f0c42389f581472d0a4967d",
      "e7d1cc5a2b0d4985b8a09e6c1a705a83",
      "652b8930669943e880ae3d89576b74fc",
      "98960d3a36a94c3aaef13a654822df71",
      "933290c8337643029d281bcda9dd979d",
      "ce31a2e9755848b480797fefbac29834",
      "4e99c1468be7438c8376481cbb2fd392",
      "cea1bd314fb546d185e89b6b2d7a2f55",
      "f081b44e13104762b0040f3288b51ad9",
      "cae957d2a80641beb5f2c843c72bf810",
      "c2612766000842d895c3679fbc2f5456",
      "e8cefeccd3434fbfb8648cbf1b27cff9",
      "039d13464a3544a9bb1c2bd393f72e68",
      "eda3a1c9b20a43b2b41f2ee5b2fcf643",
      "90d644160eee4e1a916f28986491c45a",
      "94cc3c600af24dad8bfa6c9580c2bdc2",
      "90b98e6a017248018e3454f0edba5f70",
      "863490c0eebe415f8c9ca58c71aafaed",
      "11ad1ca021e545c3a5ab5e5648a24b1e",
      "cff298953e9d42a689a97cfec2a20ba9",
      "7c9e491b0877479689e3f50649a0c0a7",
      "c5c2a55b8dcd4c9b9a61dda19e00bf22",
      "9951b7aa8c484a058fbeb0cea00b9781",
      "65b70834bb594b44ae8208b91e5a7050",
      "1ac2a494f7f04c339e6d17a75eb2ac36",
      "3664cedb2f06419786b4d1e7a005221f",
      "52e1d3620d0341d4bcaf792ce8e9a6d6",
      "6a7c28d1319f4292a91c1fbfa2627410",
      "30830f561f424a599a9605b192a58a68",
      "785e36419bb4462ea216eefb59df655e",
      "620ff47e87eb4320be02e2bb6e5e0339",
      "a5e7b8c244f4442a920c6a7ef1c255b2",
      "ee86098849f6480fb41e7df6d309866b",
      "10ddfb23745042b982258fc6c71edb65",
      "a99b7c8da15b4e1596fb1ecc9e910b22",
      "d942c0fdc50149c4ab30ae2564593eb1",
      "ad2c6e9de87346cb995dfff826ef83e3",
      "c8238fa3fcf248568615370e5633e896",
      "f8e88c393225494592acc1d9e02c5873",
      "e1143bc2ce7b4b899158126006be9425",
      "01f256298bcd466f93abf5e93abe85cc",
      "aa37f546dae04c72b43876ace7b5a685",
      "ad9b14ada6bc4327a9b566f604c5deaa",
      "69726c8e47984c90a495ad0f6bf409c6",
      "658d448c63024a6c94abed2f8598cda9",
      "0b2d6004c2cc4fb985f6eaccd8492f35",
      "61de7c9706784d98a955ac3f71508271",
      "367e9a13f4cb4c849c013b5a7fe7053a",
      "ed133410ac0e479583b8be9978bac088",
      "bb07a37b200d438d9ea5499456d7a22c",
      "3be67b5f501642038d9d68c0fab081e4",
      "48b1e9546fb84bf580b20194a51164b9",
      "4745d5f98c0e491785867ea5af39dbb6",
      "8cf823cfeef148a0a4c0a46ad8b78cf9",
      "c418398aa3d34d5fbc0fc5774332faef",
      "d887c81509124c9bbf7dcce60fdcdb9f",
      "3ba077fc8c654d0d993b1c640c14c2e7",
      "4955465b374f4783a452ad8f81e2d654",
      "e537e9b4ac4c49c9ad2b5ae439a97a8b",
      "2fd45e6a055b43bb84439e1bd0a4306a",
      "bc4507960dca4ce2bbad01ff99bb1392",
      "3a3540ac4d174627993694fb981b410a",
      "c97e249d0a8a4370a56caaace60e512f"
     ]
    },
    "id": "o1tVhfHzssuH",
    "outputId": "81e5a0bf-04df-407e-bafc-e9bb0c8c3c45"
   },
   "outputs": [],
   "source": [
    "if HF_TOKEN != \"YOUR_HF_TOKEN_HERE\":\n",
    "    print(\"üì¶ CREATING MERGED MEDICAL MODEL...\")\n",
    "\n",
    "    # Save merged model (16-bit)\n",
    "    model.save_pretrained_merged(\"medical_qa_merged\", tokenizer, save_method = \"merged_16bit\")\n",
    "    model.push_to_hub_merged(\"Laksh99/gemma-3-270m-medical-qa\", tokenizer, save_method = \"merged_16bit\", token = HF_TOKEN)\n",
    "\n",
    "    print(\"‚úÖ Merged medical model saved and pushed!\")\n",
    "    print(\"üîó Available at: https://huggingface.co/Laksh99/gemma-3-270m-medical-qa\")\n",
    "    print(\"üìã Use case: Production deployment, GPU inference\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping merged model - token required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GECyD9WFswZz"
   },
   "source": [
    "**17: Create GGUF Model for LM Studio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lwxc_9O62ZxG"
   },
   "outputs": [],
   "source": [
    "!pip install mistral-common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "94f6c6fb66aa4de1bd8bd899c10f5c80",
      "14c5575841944672aac9d711da059fee",
      "f6ff5bc04fee4e5783387a92fd8f5ab6",
      "62e9c9e0830142dba5cce155b04cdcde",
      "5ea0a357ec144f22b1a28f8e091dc8ab",
      "d0f497db1bd2497b8913ff0b2bc3bdec",
      "ab8f8314dd574edaad318fdb95fcf134",
      "c2c2c1dfe7974149a5cfb463ed3b564d",
      "f4668fe3a61544d9ae9cd0d542f057bf",
      "a6b028dbc3c14138b6c6ffed1e441a96",
      "472759991b5c41fb908c44eb521e6bbf"
     ]
    },
    "id": "G4yKpJYF0Fm6",
    "outputId": "7cb6e28e-fe56-474c-adf5-03f8ab164ecf"
   },
   "outputs": [],
   "source": [
    "print(\"‚ö° CREATING MERGED MODEL FOR GGUF CONVERSION...\")\n",
    "\n",
    "# First, save the merged 16-bit model to the directory\n",
    "model.save_pretrained_merged(\n",
    "    \"medical_qa_gguf\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\"  # Recommended for GGUF\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Merged model saved successfully!\")\n",
    "\n",
    "print(\"‚ö° CREATING GGUF MODEL FOR LM STUDIO...\")\n",
    "\n",
    "# Now convert the merged model directory to GGUF without passing tokenizer\n",
    "model.save_pretrained_gguf(\n",
    "    \"medical_qa_gguf\",\n",
    "    quantization_method=\"Q8_0\"  # Best quality for LM Studio\n",
    ")\n",
    "\n",
    "print(\"‚úÖ GGUF medical model created successfully!\")\n",
    "print(\"üìÅ Local path: medical_qa_gguf/model-unsloth-Q8_0.gguf\")\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "if os.path.exists(\"medical_qa_gguf/model-unsloth-Q8_0.gguf\"):\n",
    "    file_size = os.path.getsize(\"medical_qa_gguf/model-unsloth-Q8_0.gguf\") / (1024 * 1024)\n",
    "    print(f\"üíæ File size: {file_size:.1f} MB\")\n",
    "    print(f\"üéØ Optimized for: CPU inference, offline use\")\n",
    "\n",
    "# Optional: Push to HF if token is provided\n",
    "if HF_TOKEN != \"HF_token\":\n",
    "    print(\"\\nüîÑ Also pushing to Hugging Face as backup...\")\n",
    "    model.push_to_hub_gguf(\n",
    "        \"medical_qa_gguf\",\n",
    "        quantization_method=\"Q8_0\",\n",
    "        repo_id=\"Laksh99/gemma-3-270m-medical-qa-gguf\",\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "    print(\"‚úÖ GGUF also available at: https://huggingface.co/Laksh99/gemma-3-270m-medical-qa-gguf\")\n",
    "\n",
    "print(f\"\\nüñ•Ô∏è  LM STUDIO SETUP GUIDE:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Download LM Studio: https://lmstudio.ai/\")\n",
    "print(\"2. Install and open LM Studio\")\n",
    "print(\"3. Click 'Load Model' ‚Üí 'Load from file'\")\n",
    "print(\"4. Select: medical_qa_gguf/model-unsloth-Q8_0.gguf\")\n",
    "print(\"5. Configure settings (see Cell 22 for optimal settings)\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  RECOMMENDED LM STUDIO SETTINGS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚Ä¢ Temperature: 0.3 (medical accuracy)\")\n",
    "print(\"‚Ä¢ Max Tokens: 200\")\n",
    "print(\"‚Ä¢ Top P: 0.9\")\n",
    "print(\"‚Ä¢ System Prompt: 'You are a medical assistant trained on medical exam data. Provide accurate information and always recommend consulting healthcare professionals.'\")\n",
    "\n",
    "print(f\"\\nüì± COMPATIBLE WITH:\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ LM Studio (recommended)\")\n",
    "print(\"‚úÖ Ollama\")\n",
    "print(\"‚úÖ llama.cpp\")\n",
    "print(\"‚úÖ GPT4All\")\n",
    "print(\"‚úÖ Jan.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AeuPgCZs6Hq"
   },
   "source": [
    "**18: Download GGUF Model from Colab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "QitJ26YktAkb",
    "outputId": "f7bf770c-ca58-4591-b502-ef2fe1eaaeb9"
   },
   "outputs": [],
   "source": [
    "print(\"üì• DOWNLOAD GGUF MODEL FOR LM STUDIO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if GGUF file exists\n",
    "import os\n",
    "gguf_path = \"/content/medical_qa_gguf.Q8_0.gguf\"\n",
    "\n",
    "if os.path.exists(gguf_path):\n",
    "    file_size = os.path.getsize(gguf_path) / (1024 * 1024)  # MB\n",
    "    print(f\"‚úÖ GGUF model ready for download!\")\n",
    "    print(f\"üìÅ File: {gguf_path}\")\n",
    "    print(f\"üíæ Size: {file_size:.1f} MB\")\n",
    "\n",
    "    print(f\"\\nüì• DOWNLOAD OPTIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Option 1: Right-click file in Colab browser ‚Üí Download\")\n",
    "    print(\"Option 2: Use the download command below\")\n",
    "\n",
    "    print(f\"\\n‚¨áÔ∏è  EXECUTE THIS TO DOWNLOAD:\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Provide download functionality\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print(\"üîÑ Initiating download...\")\n",
    "        files.download(gguf_path)\n",
    "        print(\"‚úÖ Download started! Check your Downloads folder.\")\n",
    "    except ImportError:\n",
    "        print(\"‚ÑπÔ∏è  Not in Colab environment. File available at:\", gguf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Download error: {e}\")\n",
    "        print(\"üí° Try right-clicking the file in Colab file browser\")\n",
    "\n",
    "    print(f\"\\nüéØ AFTER DOWNLOAD:\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"1. Open LM Studio on your computer\")\n",
    "    print(\"2. Load the downloaded .gguf file\")\n",
    "    print(\"3. Configure medical assistant settings\")\n",
    "    print(\"4. Start asking medical questions!\")\n",
    "\n",
    "    print(f\"\\nüè• SAMPLE MEDICAL QUESTIONS TO TRY:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"‚Ä¢ What are the symptoms of diabetes?\")\n",
    "    print(\"‚Ä¢ How is hypertension diagnosed?\")\n",
    "    print(\"‚Ä¢ What are the side effects of metformin?\")\n",
    "    print(\"‚Ä¢ When should someone see a cardiologist?\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå GGUF file not found!\")\n",
    "    print(\"üîß Please run Cell 17 first to create the GGUF model.\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  IMPORTANT REMINDER:\")\n",
    "print(\"=\"*60)\n",
    "print(\"This model is for educational purposes only.\")\n",
    "print(\"Always recommend consulting healthcare professionals.\")\n",
    "print(\"Not suitable for emergency medical situations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4mYZx-ntDDA"
   },
   "source": [
    "**19: Final Summary and Next Steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1feDi5tMtHxo",
    "outputId": "6ca347fe-0e7c-4f99-acf2-cad43975a40c"
   },
   "outputs": [],
   "source": [
    "print(\"üéâ MEDICAL Q&A MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"üìä TRAINING SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚úÖ Dataset: MedMCQA (15,000 Indian medical exam questions)\")\n",
    "print(f\"‚úÖ Model: Gemma-3 270M with Medical LoRA\")\n",
    "print(f\"‚úÖ Training completed successfully\")\n",
    "print(f\"‚úÖ GGUF model created for local deployment\")\n",
    "\n",
    "print(f\"\\nüì¶ MODEL FILES CREATED:\")\n",
    "print(\"=\"*70)\n",
    "print(\"üîπ Local LoRA: medical_qa_lora/\")\n",
    "print(\"üîπ Local GGUF: medical_qa_gguf/model-unsloth-Q8_0.gguf\")\n",
    "\n",
    "if HF_TOKEN != \"YOUR_HF_TOKEN_HERE\":\n",
    "    print(\"üîπ HF LoRA: Laksh99/gemma-3-270m-medical-qa-lora\")\n",
    "    print(\"üîπ HF Merged: Laksh99/gemma-3-270m-medical-qa\")\n",
    "    print(\"üîπ HF GGUF: Laksh99/gemma-3-270m-medical-qa-gguf\")\n",
    "\n",
    "print(f\"\\nüñ•Ô∏è  LM STUDIO DEPLOYMENT:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. ‚úÖ Download the GGUF file (Cell 18)\")\n",
    "print(\"2. ‚úÖ Install LM Studio from https://lmstudio.ai/\")\n",
    "print(\"3. ‚úÖ Load the model in LM Studio\")\n",
    "print(\"4. ‚úÖ Configure medical assistant settings\")\n",
    "print(\"5. ‚úÖ Start medical consultations!\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  OPTIMAL LM STUDIO CONFIGURATION:\")\n",
    "print(\"=\"*70)\n",
    "print(\"üî∏ Temperature: 0.3 (for medical accuracy)\")\n",
    "print(\"üî∏ Max Tokens: 200\")\n",
    "print(\"üî∏ Top P: 0.9\")\n",
    "print(\"üî∏ System Prompt: Medical assistant with disclaimers\")\n",
    "\n",
    "print(f\"\\nüè• MEDICAL USE CASES:\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ Medical exam preparation and study\")\n",
    "print(\"‚úÖ Quick medical reference for healthcare workers\")\n",
    "print(\"‚úÖ Educational tool for medical students\")\n",
    "print(\"‚úÖ Basic medical information (with professional consultation)\")\n",
    "print(\"‚úÖ Rural clinic support (offline capability)\")\n",
    "\n",
    "print(f\"\\nüö® IMPORTANT MEDICAL DISCLAIMERS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚ö†Ô∏è  Educational/reference use only - not for diagnosis\")\n",
    "print(\"‚ö†Ô∏è  Always recommend consulting healthcare professionals\")\n",
    "print(\"‚ö†Ô∏è  Emergency cases: direct to emergency services\")\n",
    "print(\"‚ö†Ô∏è  Verify all medical information with qualified doctors\")\n",
    "\n",
    "print(f\"\\nüéØ SUCCESS METRICS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"üìà Expected accuracy: 75-85% on medical questions\")\n",
    "print(\"‚ö° Response time: 1-3 seconds on modern hardware\")\n",
    "print(\"üíæ Memory usage: 4-8GB RAM\")\n",
    "print(\"üì± File size: ~270MB (portable)\")\n",
    "\n",
    "print(f\"\\nüöÄ READY FOR DEPLOYMENT!\")\n",
    "print(\"=\"*70)\n",
    "print(\"Your medical Q&A assistant is ready to help improve\")\n",
    "print(\"healthcare access and medical education!\")\n",
    "\n",
    "print(f\"\\nüìö NEXT STEPS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. Download GGUF model from Cell 18\")\n",
    "print(\"2. Set up LM Studio on your computer\")\n",
    "print(\"3. Test with sample medical questions\")\n",
    "print(\"4. Deploy in your target environment\")\n",
    "print(\"5. Gather feedback and improve\")\n",
    "\n",
    "print(\"\\nüèÜ Congratulations on creating a medical AI assistant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlPeQ043tJhX"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

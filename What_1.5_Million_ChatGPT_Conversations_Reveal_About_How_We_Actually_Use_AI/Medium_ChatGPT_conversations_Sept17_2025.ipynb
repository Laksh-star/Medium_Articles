{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXUmXcT7ECfB"
      },
      "outputs": [],
      "source": [
        "# üìä ChatGPT Personal Usage Analysis\n",
        "\n",
        "This notebook analyzes your ChatGPT conversation data to provide insights into your usage patterns, including:\n",
        "\n",
        "- **Work vs Non-work classification** of your messages\n",
        "- **Intent analysis** (Asking, Doing, Expressing)\n",
        "- **Topic categorization** across 15+ categories\n",
        "- **Usage patterns** by time, day, and conversation length\n",
        "- **Comparison** with research findings\n",
        "\n",
        "## üî¨ Research Background\n",
        "\n",
        "This analysis is based on the methodology from the research paper: *\"How Do People Use ChatGPT? Analyzing User Behavior and Message Content\"* by Zhang et al. (2024).\n",
        "\n",
        "## üìà Expected Results\n",
        "\n",
        "- Most users have ~27% work-related messages\n",
        "- ~49% of messages are \"Asking\" for information\n",
        "- Common topics include Writing, Technical Help, and Practical Guidance\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup Instructions**\n",
        "Step 1: Get Your ChatGPT Export\n",
        "\n",
        "Go to ChatGPT\n",
        "Click your profile icon ‚Üí Settings ‚Üí Data Controls\n",
        "Click \"Export\" next to \"Export data\"\n",
        "Check your email for the download link\n",
        "Download and extract the ZIP file\n",
        "Locate conversations.json inside\n",
        "\n",
        "Step 2: Set Up Google Colab\n",
        "\n",
        "Open Google Colab\n",
        "Create a new notebook\n",
        "Copy-paste the cells below in order\n",
        "\n",
        "Step 3: Set Up OpenAI API (Optional but Recommended)\n",
        "\n",
        "Get API key from OpenAI Platform\n",
        "In Colab, click the üîë key icon on the left sidebar\n",
        "Add new secret: Name = OPENAI_API_KEY, Value = your API key:"
      ],
      "metadata": {
        "id": "492Y6fVmEF4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Step 1: Install Dependencies\n",
        "\n",
        "This cell installs all required Python packages for the analysis. Run this first!"
      ],
      "metadata": {
        "id": "gCSuHEYFEcvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install openai pandas matplotlib seaborn numpy -q\n",
        "\n",
        "# Restart runtime after installation (only needed once)\n",
        "import sys\n",
        "import subprocess\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openai\", \"pandas\", \"matplotlib\", \"seaborn\", \"numpy\"])\n",
        "\n",
        "print(\"‚úÖ Dependencies installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQgIJGWSEatk",
        "outputId": "1f826838-3771-4025-9c30-3a02b1b1ea76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dependencies installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Step 2: Import Libraries\n",
        "\n",
        "This cell imports all necessary libraries for data processing, visualization, and OpenAI API integration."
      ],
      "metadata": {
        "id": "vGEjfakqEu6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timezone\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Colab-specific imports\n",
        "from google.colab import userdata, files\n",
        "\n",
        "# OpenAI imports\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    OPENAI_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OPENAI_AVAILABLE = False\n",
        "    print(\"OpenAI library not available\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(\"\\nüìã NEXT STEPS:\")\n",
        "print(\"1. Run the ChatGPTPersonalAnalyzer class cell\")\n",
        "print(\"2. Upload your conversations.json file\")\n",
        "print(\"3. Run the analysis\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt8vXAo0EvUz",
        "outputId": "06120f3c-3d0f-49c2-8c89-3e0181d6bd4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries imported successfully!\n",
            "\n",
            "üìã NEXT STEPS:\n",
            "1. Run the ChatGPTPersonalAnalyzer class cell\n",
            "2. Upload your conversations.json file\n",
            "3. Run the analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug the role field in your messages\n",
        "def debug_roles(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        conversations = json.load(file)\n",
        "\n",
        "    print(\"üîç DEBUGGING MESSAGE ROLES\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    role_examples = {}\n",
        "\n",
        "    for i, conv in enumerate(conversations[:5]):  # Check first 5 conversations\n",
        "        chat_messages = conv.get('chat_messages', [])\n",
        "        print(f\"\\nConversation {i+1} - {len(chat_messages)} messages:\")\n",
        "\n",
        "        for j, msg in enumerate(chat_messages[:3]):  # First 3 messages each\n",
        "            print(f\"  Message {j+1}:\")\n",
        "            print(f\"    All keys: {list(msg.keys())}\")\n",
        "\n",
        "            # Check different possible role fields\n",
        "            role_fields = ['role', 'author', 'sender', 'from', 'type']\n",
        "            for field in role_fields:\n",
        "                if field in msg:\n",
        "                    value = msg[field]\n",
        "                    print(f\"    {field}: {value}\")\n",
        "                    if field not in role_examples:\n",
        "                        role_examples[field] = set()\n",
        "                    role_examples[field].add(str(value))\n",
        "\n",
        "            # Show content preview\n",
        "            if 'content' in msg:\n",
        "                content = str(msg['content'])[:100]\n",
        "                print(f\"    Content preview: {content}...\")\n",
        "            print()\n",
        "\n",
        "    print(f\"\\nRole field summary:\")\n",
        "    for field, values in role_examples.items():\n",
        "        print(f\"  {field}: {list(values)}\")\n",
        "\n",
        "# Run the debug\n",
        "debug_roles('conversations.json')"
      ],
      "metadata": {
        "id": "nTO6ANsuS8E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Step 4: Analysis Class Definition\n",
        "\n",
        "This section contains the main analyzer class with two versions:\n",
        "\n",
        "1. **Basic version** (Cell below): Uses keyword-based heuristics\n",
        "2. **Advanced version** (Next section): Uses OpenAI API for accurate classification\n",
        "\n",
        "### Basic Analyzer (Heuristic-based)"
      ],
      "metadata": {
        "id": "WpFgO2yA-Sq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 : Main Analyzer Class"
      ],
      "metadata": {
        "id": "lCGVnrwNEyUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedCustomFormatAnalyzer:\n",
        "    def __init__(self, api_key=None):\n",
        "        \"\"\"Initialize analyzer for your specific export format\"\"\"\n",
        "        self.conversations = []\n",
        "        self.messages_df = None\n",
        "        self.user_messages_df = None\n",
        "        self.classified_messages = None\n",
        "\n",
        "        try:\n",
        "            from openai import OpenAI\n",
        "            if api_key:\n",
        "                self.client = OpenAI(api_key=api_key)\n",
        "                print(\"‚úÖ OpenAI client initialized\")\n",
        "            else:\n",
        "                self.client = None\n",
        "                print(\"‚ö†Ô∏è Using heuristic classifications\")\n",
        "        except ImportError:\n",
        "            self.client = None\n",
        "            print(\"‚ö†Ô∏è OpenAI library not available\")\n",
        "\n",
        "    def load_conversations(self, file_path):\n",
        "        \"\"\"Load conversations from your specific export format\"\"\"\n",
        "        print(f\"Loading conversations from {file_path}...\")\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            self.conversations = json.load(file)\n",
        "\n",
        "        print(f\"‚úÖ Loaded {len(self.conversations)} conversations\")\n",
        "        return self\n",
        "\n",
        "    def extract_messages(self):\n",
        "        \"\"\"Extract messages from your chat_messages format\"\"\"\n",
        "        print(\"Extracting messages from conversations...\")\n",
        "\n",
        "        messages = []\n",
        "\n",
        "        for conv_idx, conversation in enumerate(self.conversations):\n",
        "            # Extract conversation metadata\n",
        "            conv_id = conversation.get('uuid', f'conv_{conv_idx}')\n",
        "            title = conversation.get('name', 'Untitled')\n",
        "            created_at = conversation.get('created_at', '')\n",
        "            updated_at = conversation.get('updated_at', '')\n",
        "            chat_messages = conversation.get('chat_messages', [])\n",
        "\n",
        "            # Convert timestamps\n",
        "            try:\n",
        "                if created_at:\n",
        "                    create_time = datetime.fromisoformat(created_at.replace('Z', '+00:00')).timestamp()\n",
        "                else:\n",
        "                    create_time = 0\n",
        "            except:\n",
        "                create_time = 0\n",
        "\n",
        "            try:\n",
        "                if updated_at:\n",
        "                    update_time = datetime.fromisoformat(updated_at.replace('Z', '+00:00')).timestamp()\n",
        "                else:\n",
        "                    update_time = 0\n",
        "            except:\n",
        "                update_time = 0\n",
        "\n",
        "            # Extract messages from chat_messages array\n",
        "            for msg_idx, message in enumerate(chat_messages):\n",
        "                # Extract role - convert 'human' to 'user' for consistency\n",
        "                sender = message.get('sender', 'unknown')\n",
        "                if sender == 'human':\n",
        "                    role = 'user'\n",
        "                elif sender == 'assistant' or sender == 'ai':\n",
        "                    role = 'assistant'\n",
        "                else:\n",
        "                    role = sender\n",
        "\n",
        "                # Extract content - handle complex content structure\n",
        "                content_raw = message.get('content', '')\n",
        "                text_content = \"\"\n",
        "\n",
        "                if isinstance(content_raw, list):\n",
        "                    # Extract text from list of content objects\n",
        "                    for item in content_raw:\n",
        "                        if isinstance(item, dict):\n",
        "                            # Look for text in various possible fields\n",
        "                            if 'text' in item:\n",
        "                                text_content += str(item['text']) + \" \"\n",
        "                            elif 'content' in item:\n",
        "                                text_content += str(item['content']) + \" \"\n",
        "                        else:\n",
        "                            text_content += str(item) + \" \"\n",
        "                elif isinstance(content_raw, dict):\n",
        "                    # Single content object\n",
        "                    if 'text' in content_raw:\n",
        "                        text_content = str(content_raw['text'])\n",
        "                    elif 'content' in content_raw:\n",
        "                        text_content = str(content_raw['content'])\n",
        "                    else:\n",
        "                        text_content = str(content_raw)\n",
        "                else:\n",
        "                    # Simple string content\n",
        "                    text_content = str(content_raw)\n",
        "\n",
        "                # Also check the 'text' field directly\n",
        "                if 'text' in message and message['text']:\n",
        "                    if not text_content.strip():\n",
        "                        text_content = str(message['text'])\n",
        "\n",
        "                # Get message timestamp\n",
        "                msg_created_at = message.get('created_at', created_at)\n",
        "                try:\n",
        "                    if msg_created_at:\n",
        "                        msg_time = datetime.fromisoformat(msg_created_at.replace('Z', '+00:00')).timestamp()\n",
        "                    else:\n",
        "                        msg_time = create_time\n",
        "                except:\n",
        "                    msg_time = create_time\n",
        "\n",
        "                # Only add messages with actual text content\n",
        "                if text_content and text_content.strip():\n",
        "                    msg_data = {\n",
        "                        'conversation_id': conv_id,\n",
        "                        'conversation_title': title,\n",
        "                        'conversation_create_time': create_time,\n",
        "                        'conversation_update_time': update_time,\n",
        "                        'message_id': f\"{conv_id}_{msg_idx}\",\n",
        "                        'author_role': role,\n",
        "                        'content': text_content.strip(),\n",
        "                        'message_create_time': msg_time,\n",
        "                        'content_type': 'text',\n",
        "                        'word_count': len(text_content.strip().split()),\n",
        "                        'char_count': len(text_content.strip())\n",
        "                    }\n",
        "                    messages.append(msg_data)\n",
        "\n",
        "        self.messages_df = pd.DataFrame(messages)\n",
        "        print(f\"‚úÖ Extracted {len(messages)} messages from {len(self.conversations)} conversations\")\n",
        "\n",
        "        if not self.messages_df.empty:\n",
        "            self._add_derived_features()\n",
        "        else:\n",
        "            self.user_messages_df = pd.DataFrame()\n",
        "            print(\"‚ùå No messages extracted\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _add_derived_features(self):\n",
        "        \"\"\"Add derived features to the messages dataframe\"\"\"\n",
        "        # Convert timestamps\n",
        "        self.messages_df['create_datetime'] = pd.to_datetime(\n",
        "            self.messages_df['message_create_time'], unit='s', utc=True\n",
        "        )\n",
        "        self.messages_df['conversation_create_datetime'] = pd.to_datetime(\n",
        "            self.messages_df['conversation_create_time'], unit='s', utc=True\n",
        "        )\n",
        "\n",
        "        # Extract date components\n",
        "        self.messages_df['date'] = self.messages_df['create_datetime'].dt.date\n",
        "        self.messages_df['hour'] = self.messages_df['create_datetime'].dt.hour\n",
        "        self.messages_df['day_of_week'] = self.messages_df['create_datetime'].dt.day_name()\n",
        "        self.messages_df['month'] = self.messages_df['create_datetime'].dt.month\n",
        "        self.messages_df['year'] = self.messages_df['create_datetime'].dt.year\n",
        "\n",
        "        # Filter to user messages\n",
        "        self.user_messages_df = self.messages_df[\n",
        "            self.messages_df['author_role'] == 'user'\n",
        "        ].copy()\n",
        "\n",
        "        print(f\"   Total messages: {len(self.messages_df)}\")\n",
        "        print(f\"   Your messages: {len(self.user_messages_df)}\")\n",
        "        print(f\"   Role distribution: {dict(self.messages_df['author_role'].value_counts())}\")\n",
        "\n",
        "    # [Include all the classification methods from the previous analyzer - _classify_work_related, _classify_intent, _classify_topic, etc.]\n",
        "    def _get_message_context(self, message_row):\n",
        "        \"\"\"Get context for a message\"\"\"\n",
        "        conv_id = message_row['conversation_id']\n",
        "        msg_time = message_row['message_create_time']\n",
        "\n",
        "        conv_messages = self.messages_df[\n",
        "            (self.messages_df['conversation_id'] == conv_id) &\n",
        "            (self.messages_df['message_create_time'] <= msg_time)\n",
        "        ].sort_values('message_create_time')\n",
        "\n",
        "        context_messages = conv_messages.tail(10)\n",
        "\n",
        "        context = []\n",
        "        for _, ctx_msg in context_messages.iterrows():\n",
        "            role = ctx_msg['author_role']\n",
        "            content = ctx_msg['content'][:1000]\n",
        "            context.append(f\"[{role}]: {content}\")\n",
        "\n",
        "        return \"\\n\".join(context)\n",
        "\n",
        "    def _classify_work_related(self, context):\n",
        "        # [Same as before - work classification logic]\n",
        "        work_keywords = ['work', 'job', 'office', 'meeting', 'project', 'client', 'business',\n",
        "                        'professional', 'career', 'resume', 'email', 'report', 'presentation']\n",
        "        content_lower = context.lower()\n",
        "        return int(any(keyword in content_lower for keyword in work_keywords))\n",
        "\n",
        "    def _classify_intent(self, context):\n",
        "        # [Same as before - intent classification logic]\n",
        "        last_message = context.split('\\n')[-1] if context else \"\"\n",
        "        if any(word in last_message.lower() for word in ['what', 'how', 'why', 'when', 'where', 'who', '?']):\n",
        "            return 'Asking'\n",
        "        elif any(word in last_message.lower() for word in ['write', 'create', 'make', 'generate', 'draft', 'rewrite']):\n",
        "            return 'Doing'\n",
        "        else:\n",
        "            return 'Expressing'\n",
        "\n",
        "    def _classify_topic(self, context):\n",
        "        # [Same as before - topic classification logic]\n",
        "        topics = {\n",
        "            'Writing': ['write', 'edit', 'rewrite', 'draft', 'essay', 'email', 'letter'],\n",
        "            'Practical Guidance': ['how to', 'help me', 'advice', 'guide', 'tutorial', 'tips'],\n",
        "            'Seeking Information': ['what is', 'tell me about', 'explain', 'define', 'information'],\n",
        "            'Technical Help': ['code', 'programming', 'python', 'javascript', 'debug', 'error'],\n",
        "            'Creative': ['story', 'poem', 'creative', 'fiction', 'art', 'design'],\n",
        "            'Self-Expression': ['feel', 'think', 'opinion', 'personal', 'relationship'],\n",
        "        }\n",
        "\n",
        "        last_message = context.split('\\n')[-1].lower() if context else \"\"\n",
        "        for topic, keywords in topics.items():\n",
        "            if any(keyword in last_message for keyword in keywords):\n",
        "                return topic\n",
        "        return 'Other'\n",
        "\n",
        "    def classify_messages(self, sample_size=None):\n",
        "        \"\"\"Classify messages using the paper's methodology\"\"\"\n",
        "        if len(self.user_messages_df) == 0:\n",
        "            print(\"‚ùå No user messages to classify\")\n",
        "            return self\n",
        "\n",
        "        print(\"Starting message classification...\")\n",
        "\n",
        "        if sample_size and len(self.user_messages_df) > sample_size:\n",
        "            messages_to_classify = self.user_messages_df.sample(sample_size, random_state=42)\n",
        "            print(f\"üìä Sampling {sample_size} messages for classification\")\n",
        "        else:\n",
        "            messages_to_classify = self.user_messages_df.copy()\n",
        "\n",
        "        classified_results = []\n",
        "\n",
        "        for idx, row in messages_to_classify.iterrows():\n",
        "            if len(classified_results) % 50 == 0:\n",
        "                print(f\"   Classifying message {len(classified_results) + 1}/{len(messages_to_classify)}...\")\n",
        "\n",
        "            context = self._get_message_context(row)\n",
        "\n",
        "            classifications = {\n",
        "                'message_idx': idx,\n",
        "                'conversation_id': row['conversation_id'],\n",
        "                'message_id': row['message_id'],\n",
        "                'content': row['content'][:200] + '...' if len(row['content']) > 200 else row['content'],\n",
        "                'create_datetime': row['create_datetime'],\n",
        "                'word_count': row['word_count'],\n",
        "                'is_work': self._classify_work_related(context),\n",
        "                'intent': self._classify_intent(context),\n",
        "                'topic': self._classify_topic(context)\n",
        "            }\n",
        "\n",
        "            classified_results.append(classifications)\n",
        "\n",
        "        self.classified_messages = pd.DataFrame(classified_results)\n",
        "        print(f\"‚úÖ Completed classification of {len(classified_results)} messages\")\n",
        "        return self\n",
        "\n",
        "    def generate_analysis(self):\n",
        "        \"\"\"Generate comprehensive analysis\"\"\"\n",
        "        if self.classified_messages is None or len(self.classified_messages) == 0:\n",
        "            print(\"‚ùå No classified messages available. Run classify_messages() first.\")\n",
        "            return\n",
        "\n",
        "        print(\"üìä Generating analysis report...\")\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "        # Work vs Non-work\n",
        "        work_counts = self.classified_messages['is_work'].value_counts()\n",
        "        axes[0,0].pie([work_counts.get(0, 0), work_counts.get(1, 0)],\n",
        "                     labels=['Non-Work', 'Work'], autopct='%1.1f%%')\n",
        "        axes[0,0].set_title('Work vs Non-Work Messages')\n",
        "\n",
        "        # Intent distribution\n",
        "        intent_counts = self.classified_messages['intent'].value_counts()\n",
        "        axes[0,1].bar(intent_counts.index, intent_counts.values)\n",
        "        axes[0,1].set_title('User Intent Distribution')\n",
        "        axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Topic distribution\n",
        "        topic_counts = self.classified_messages['topic'].value_counts()\n",
        "        axes[0,2].bar(topic_counts.index, topic_counts.values)\n",
        "        axes[0,2].set_title('Topic Distribution')\n",
        "        axes[0,2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Hourly usage\n",
        "        hourly_counts = self.user_messages_df.groupby('hour').size()\n",
        "        axes[1,0].bar(hourly_counts.index, hourly_counts.values)\n",
        "        axes[1,0].set_title('Usage by Hour')\n",
        "\n",
        "        # Daily usage\n",
        "        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "        daily_counts = self.user_messages_df.groupby('day_of_week').size().reindex(day_order, fill_value=0)\n",
        "        axes[1,1].bar(daily_counts.index, daily_counts.values)\n",
        "        axes[1,1].set_title('Usage by Day of Week')\n",
        "        axes[1,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Message length\n",
        "        axes[1,2].hist(self.user_messages_df['word_count'], bins=30, alpha=0.7)\n",
        "        axes[1,2].set_title('Message Length Distribution')\n",
        "        axes[1,2].set_xlabel('Word Count')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        self._print_summary()\n",
        "\n",
        "    def _print_summary(self):\n",
        "        \"\"\"Print summary statistics\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"CHATGPT PERSONAL USAGE ANALYSIS SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        print(f\"\\nüìä BASIC STATISTICS:\")\n",
        "        print(f\"   Total Conversations: {len(self.conversations):,}\")\n",
        "        print(f\"   Total Messages: {len(self.messages_df):,}\")\n",
        "        print(f\"   Your Messages: {len(self.user_messages_df):,}\")\n",
        "        print(f\"   Classified Messages: {len(self.classified_messages):,}\")\n",
        "\n",
        "        if len(self.user_messages_df) > 0:\n",
        "            start_date = self.user_messages_df['create_datetime'].min()\n",
        "            end_date = self.user_messages_df['create_datetime'].max()\n",
        "            print(f\"   Time Range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "            avg_length = self.user_messages_df['word_count'].mean()\n",
        "            print(f\"   Average Message Length: {avg_length:.1f} words\")\n",
        "\n",
        "        if len(self.classified_messages) > 0:\n",
        "            print(f\"\\nüè¢ WORK VS NON-WORK:\")\n",
        "            work_stats = self.classified_messages['is_work'].value_counts(normalize=True) * 100\n",
        "            print(f\"   Work-Related: {work_stats.get(1, 0):.1f}%\")\n",
        "            print(f\"   Non-Work: {work_stats.get(0, 0):.1f}%\")\n",
        "\n",
        "            print(f\"\\nüéØ INTENT DISTRIBUTION:\")\n",
        "            intent_stats = self.classified_messages['intent'].value_counts(normalize=True) * 100\n",
        "            for intent, pct in intent_stats.items():\n",
        "                print(f\"   {intent}: {pct:.1f}%\")\n",
        "\n",
        "            print(f\"\\nüìö TOPIC DISTRIBUTION:\")\n",
        "            topic_stats = self.classified_messages['topic'].value_counts(normalize=True) * 100\n",
        "            for topic, pct in topic_stats.items():\n",
        "                print(f\"   {topic}: {pct:.1f}%\")\n",
        "\n",
        "            print(f\"\\nüìà COMPARISON TO RESEARCH PAPER:\")\n",
        "            your_work_pct = work_stats.get(1, 0)\n",
        "            print(f\"   Your work usage: {your_work_pct:.1f}% (Paper average: ~27%)\")\n",
        "\n",
        "            if 'Asking' in intent_stats:\n",
        "                your_asking_pct = intent_stats.get('Asking', 0)\n",
        "                print(f\"   Your 'Asking' usage: {your_asking_pct:.1f}% (Paper average: ~49%)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "print(\"‚úÖ Fixed analyzer loaded!\")\n",
        "print(\"\\nNow run:\")\n",
        "print(\"analyzer = FixedCustomFormatAnalyzer()\")\n",
        "print(\"analyzer.load_conversations('conversations.json')\")\n",
        "print(\"analyzer.extract_messages()\")\n",
        "print(\"analyzer.classify_messages(sample_size=500)\")\n",
        "print(\"analyzer.generate_analysis()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-k33kmFnR7c-",
        "outputId": "112996d8-8034-459d-f83d-188b13e40a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fixed analyzer loaded!\n",
            "\n",
            "Now run:\n",
            "analyzer = FixedCustomFormatAnalyzer()\n",
            "analyzer.load_conversations('conversations.json')\n",
            "analyzer.extract_messages()\n",
            "analyzer.classify_messages(sample_size=500)\n",
            "analyzer.generate_analysis()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # def __init__(self, api_key=None):\n",
        "    #     \"\"\"Initialize analyzer for your specific export format\"\"\"\n",
        "    #     self.conversations = []\n",
        "    #     self.messages_df = None\n",
        "    #     self.user_messages_df = None\n",
        "    #     self.classified_messages = None\n",
        "\n",
        "    #     try:\n",
        "    #         from openai import OpenAI\n",
        "    #         if api_key:\n",
        "    #             self.client = OpenAI(api_key=api_key)\n",
        "    #             print(\"‚úÖ OpenAI client initialized\")\n",
        "    #         else:\n",
        "    #             self.client = None\n",
        "    #             print(\"‚ö†Ô∏è  Using heuristic classifications\")\n",
        "    #     except ImportError:\n",
        "    #         self.client = None\n",
        "    #         print(\"‚ö†Ô∏è  OpenAI library not available\")"
      ],
      "metadata": {
        "id": "hVUQRJqSWkPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Step 5: Run Basic Analysis (Heuristic-based)\n",
        "\n",
        "This cell runs the analysis using keyword-based classification. It's free but less accurate than the OpenAI version.\n",
        "\n",
        "**Note**: This uses simple keyword matching to classify your messages. For more accurate results, use the OpenAI-powered version below."
      ],
      "metadata": {
        "id": "nPvPc5W--Sq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Step 6: Advanced Analysis with OpenAI API (Recommended)\n",
        "\n",
        "This version uses OpenAI's GPT models for highly accurate message classification.\n",
        "\n",
        "### üí° Why Use OpenAI API?\n",
        "- **Much more accurate** than keyword-based classification\n",
        "- **Context-aware** - considers conversation history\n",
        "- **Consistent** with research methodology\n",
        "- **Cost**: ~$3-6 for typical analysis\n",
        "\n",
        "### üìã Setup Requirements:\n",
        "1. Get an OpenAI API key from [platform.openai.com](https://platform.openai.com)\n",
        "2. In Colab: Click üîë icon ‚Üí Add secret: `OPENAI_API_KEY` = your key\n",
        "3. Run the cell below\n",
        "\n",
        "### Advanced Analyzer (OpenAI-powered)"
      ],
      "metadata": {
        "id": "SAi81Dz4-Sq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéâ Analysis Complete!\n",
        "\n",
        "Your ChatGPT usage analysis is now complete. The results above show:\n",
        "\n",
        "### üìà Key Insights You Can Extract:\n",
        "- **Work-life balance**: How much you use ChatGPT for work vs personal\n",
        "- **Usage style**: Whether you mostly ask questions, request tasks, or express thoughts\n",
        "- **Interest areas**: Your most common topics of discussion\n",
        "- **Time patterns**: When you're most active with ChatGPT\n",
        "- **Comparison**: How you compare to typical ChatGPT users\n",
        "\n",
        "### üíæ Next Steps:\n",
        "- Screenshot or save the visualizations\n",
        "- Copy the summary statistics for your records\n",
        "- Consider adjusting your ChatGPT usage based on insights\n",
        "- Share findings (anonymized) with others interested in AI usage patterns\n",
        "\n",
        "### üî¨ Research Context:\n",
        "This analysis follows the methodology from *\"How Do People Use ChatGPT? Analyzing User Behavior and Message Content\"* - helping contribute to understanding of AI tool adoption and usage patterns.\n",
        "\n",
        "---\n",
        "\n",
        "**Questions or want to modify the analysis?** Check the GitHub repository for documentation and customization options."
      ],
      "metadata": {
        "id": "aX9fdtYS-Sq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = FixedCustomFormatAnalyzer()\n",
        "analyzer.load_conversations('conversations.json')\n",
        "analyzer.extract_messages()\n",
        "analyzer.classify_messages(sample_size=500)\n",
        "analyzer.generate_analysis()"
      ],
      "metadata": {
        "id": "MMaTDpMFSBQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with openai key - FIXED VERSION\n",
        "import os\n",
        "\n",
        "class FixedCustomFormatAnalyzer:\n",
        "    def __init__(self, api_key=None):\n",
        "        \"\"\"Initialize analyzer for your specific export format\"\"\"\n",
        "        self.conversations = []\n",
        "        self.messages_df = None\n",
        "        self.user_messages_df = None\n",
        "        self.classified_messages = None\n",
        "\n",
        "        # Try to get API key from multiple sources\n",
        "        if not api_key:\n",
        "            try:\n",
        "                # Try Google Colab userdata first\n",
        "                from google.colab import userdata\n",
        "                api_key = userdata.get('OPENAI_API_KEY')\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if not api_key:\n",
        "            # Try environment variable\n",
        "            api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "        try:\n",
        "            from openai import OpenAI\n",
        "            if api_key and api_key.strip():  # Check if key exists and is not empty\n",
        "                self.client = OpenAI(api_key=api_key.strip())\n",
        "                print(\"‚úÖ OpenAI client initialized for accurate classifications\")\n",
        "                print(f\"   Using API key: {api_key[:10]}...{api_key[-4:] if len(api_key) > 14 else ''}\")\n",
        "            else:\n",
        "                self.client = None\n",
        "                print(\"‚ö†Ô∏è Using heuristic classifications (no OpenAI API key found)\")\n",
        "        except ImportError:\n",
        "            self.client = None\n",
        "            print(\"‚ö†Ô∏è OpenAI library not available\")\n",
        "        except Exception as e:\n",
        "            self.client = None\n",
        "            print(f\"‚ö†Ô∏è OpenAI client initialization failed: {e}\")\n",
        "\n",
        "    def _classify_work_related(self, context):\n",
        "        \"\"\"Classify if message is work-related (with OpenAI API support)\"\"\"\n",
        "        if self.client:\n",
        "            prompt = \"\"\"You are an internal tool that classifies a message from a user to an AI chatbot, based on the context of the previous messages before it.\n",
        "\n",
        "Does the last user message of this conversation transcript seem likely to be related to doing some work/employment? Answer with one of the following:\n",
        "(1) likely part of work (e.g. \"rewrite this HR complaint\")\n",
        "(0) likely not part of work (e.g. \"does ice reduce pimples?\")\n",
        "\n",
        "In your response, only give the number and no other text. IE: the only acceptable responses are 1 and 0.\"\"\"\n",
        "\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": prompt},\n",
        "                        {\"role\": \"user\", \"content\": context[:4000]}  # Truncate context to avoid token limits\n",
        "                    ],\n",
        "                    max_tokens=1,\n",
        "                    temperature=0\n",
        "                )\n",
        "                result = response.choices[0].message.content.strip()\n",
        "                return int(result) if result in ['0', '1'] else 0\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è OpenAI API error for work classification: {e}\")\n",
        "                # Fall through to heuristic\n",
        "\n",
        "        # Heuristic fallback\n",
        "        work_keywords = ['work', 'job', 'office', 'meeting', 'project', 'client', 'business',\n",
        "                        'professional', 'career', 'resume', 'email', 'report', 'presentation',\n",
        "                        'colleague', 'manager', 'deadline', 'proposal', 'company', 'corporate']\n",
        "        content_lower = context.lower()\n",
        "        return int(any(keyword in content_lower for keyword in work_keywords))\n",
        "\n",
        "    def _classify_intent(self, context):\n",
        "        \"\"\"Classify user intent (with OpenAI API support)\"\"\"\n",
        "        if self.client:\n",
        "            prompt = \"\"\"You are an internal tool that classifies a message from a user to an AI chatbot, based on the context of the previous messages before it.\n",
        "\n",
        "Assign the last user message of this conversation transcript to one of the following three categories:\n",
        "\n",
        "- Asking: Asking is seeking information or advice that will help the user be better informed or make better decisions, either at work, at school, or in their personal life. (e.g. \"Who was president after Lincoln?\", \"How do I create a budget for this quarter?\")\n",
        "\n",
        "- Doing: Doing messages request that ChatGPT perform tasks for the user. User is drafting an email, writing code, etc. Classify messages as \"doing\" if they include requests for output that is created primarily by the model. (e.g. \"Rewrite this email to make it more formal\", \"Draft a report summarizing the use cases of ChatGPT\")\n",
        "\n",
        "- Expressing: Expressing statements are neither asking for information, nor for the chatbot to perform a task.\n",
        "\n",
        "Only reply with one word: Asking, Doing, or Expressing.\"\"\"\n",
        "\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": prompt},\n",
        "                        {\"role\": \"user\", \"content\": context[:4000]}\n",
        "                    ],\n",
        "                    max_tokens=10,\n",
        "                    temperature=0\n",
        "                )\n",
        "                result = response.choices[0].message.content.strip()\n",
        "                if result in ['Asking', 'Doing', 'Expressing']:\n",
        "                    return result\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Unexpected intent result from API: {result}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è OpenAI API error for intent classification: {e}\")\n",
        "\n",
        "        # Heuristic fallback\n",
        "        last_message = context.split('\\n')[-1] if context else \"\"\n",
        "        if any(word in last_message.lower() for word in ['what', 'how', 'why', 'when', 'where', 'who', '?']):\n",
        "            return 'Asking'\n",
        "        elif any(word in last_message.lower() for word in ['write', 'create', 'make', 'generate', 'draft', 'rewrite']):\n",
        "            return 'Doing'\n",
        "        else:\n",
        "            return 'Expressing'\n",
        "\n",
        "    def _classify_topic(self, context):\n",
        "        \"\"\"Classify conversation topic (enhanced with OpenAI for better accuracy)\"\"\"\n",
        "        if self.client:\n",
        "            prompt = \"\"\"You are an internal tool that classifies a message from a user to an AI chatbot based on conversation context.\n",
        "\n",
        "Classify the last user message into ONE of these categories:\n",
        "\n",
        "- Writing: Edit, critique, rewrite, draft emails, essays, documents, translations, summaries\n",
        "- Practical Guidance: How-to advice, tutorials, recommendations, step-by-step help, life advice\n",
        "- Seeking Information: Factual questions, explanations, definitions, research, current events\n",
        "- Technical Help: Programming, coding, debugging, math, data analysis, software troubleshooting\n",
        "- Creative: Stories, poems, art, design, fiction, creative projects, brainstorming ideas\n",
        "- Self-Expression: Personal feelings, relationships, opinions, casual chat, greetings\n",
        "- Learning: Educational content, explaining concepts, tutoring, academic help, study assistance\n",
        "- Planning: Project planning, scheduling, organizing, goal setting, strategy\n",
        "- Health/Wellness: Health questions, fitness advice, mental health, medical information\n",
        "- Entertainment: Games, jokes, fun activities, trivia, casual entertainment\n",
        "- Shopping/Products: Product recommendations, comparisons, purchasing decisions\n",
        "- Travel: Travel planning, destination advice, logistics, recommendations\n",
        "- Food/Cooking: Recipes, cooking advice, restaurant recommendations, nutrition\n",
        "- Spiritual: Bhagavad Gita, Swami Vivekananda, meditation, philosophy\n",
        "- Career: Job search, interview prep, professional development, workplace issues\n",
        "- Other: Anything that doesn't clearly fit the above categories\n",
        "\n",
        "Only reply with one category name from the list above.\"\"\"\n",
        "\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=\"gpt-4o-mini\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": prompt},\n",
        "                        {\"role\": \"user\", \"content\": context[:4000]}\n",
        "                    ],\n",
        "                    max_tokens=20,\n",
        "                    temperature=0\n",
        "                )\n",
        "                result = response.choices[0].message.content.strip()\n",
        "                valid_topics = ['Writing', 'Practical Guidance', 'Seeking Information', 'Technical Help',\n",
        "                'Creative', 'Self-Expression', 'Learning', 'Planning', 'Health/Wellness',\n",
        "                'Entertainment', 'Shopping/Products', 'Travel', 'Food/Cooking', 'Spiritual', 'Career', 'Other']\n",
        "                if result in valid_topics:\n",
        "                    return result\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Unexpected topic result from API: {result}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è OpenAI API error for topic classification: {e}\")\n",
        "\n",
        "        # Heuristic fallback\n",
        "        topics = {\n",
        "            'Writing': ['write', 'edit', 'rewrite', 'draft', 'essay', 'email', 'letter', 'document', 'summary'],\n",
        "            'Practical Guidance': ['how to', 'help me', 'advice', 'guide', 'tutorial', 'tips', 'recommend', 'should i'],\n",
        "            'Seeking Information': ['what is', 'tell me about', 'explain', 'define', 'information', 'who is', 'when did', 'where is'],\n",
        "            'Technical Help': ['code', 'programming', 'python', 'javascript', 'debug', 'error', 'technical', 'software', 'computer'],\n",
        "            'Creative': ['story', 'poem', 'creative', 'fiction', 'art', 'design', 'imagine', 'brainstorm', 'idea'],\n",
        "            'Self-Expression': ['feel', 'think', 'opinion', 'personal', 'relationship', 'emotion', 'hello', 'hi'],\n",
        "            'Learning': ['learn', 'teach', 'explain', 'understand', 'study', 'education', 'school', 'homework'],\n",
        "            'Planning': ['plan', 'schedule', 'organize', 'goal', 'strategy', 'timeline', 'project'],\n",
        "            'Health/Wellness': ['health', 'fitness', 'exercise', 'mental health', 'therapy', 'medical', 'doctor'],\n",
        "            'Entertainment': ['game', 'fun', 'joke', 'play', 'entertainment', 'movie', 'music', 'book'],\n",
        "            'Shopping/Products': ['buy', 'purchase', 'product', 'review', 'compare', 'shopping', 'price'],\n",
        "            'Travel': ['travel', 'trip', 'vacation', 'flight', 'hotel', 'destination', 'visit'],\n",
        "            'Food/Cooking': ['recipe', 'cook', 'food', 'restaurant', 'meal', 'nutrition', 'diet'],\n",
        "            'Spiritual': ['life', 'peace', 'meaning', 'spiritual', 'meditation', 'philosophy', 'bhagavad', 'vivekananda'],\n",
        "            'Career': ['job', 'career', 'interview', 'resume', 'workplace', 'professional development']\n",
        "        }\n",
        "\n",
        "        last_message = context.split('\\n')[-1].lower() if context else \"\"\n",
        "        for topic, keywords in topics.items():\n",
        "            if any(keyword in last_message for keyword in keywords):\n",
        "                return topic\n",
        "        return 'Other'\n",
        "\n",
        "    # [Keep all other existing methods: load_conversations, extract_messages, etc.]\n",
        "    def load_conversations(self, file_path):\n",
        "        \"\"\"Load conversations from your specific export format\"\"\"\n",
        "        print(f\"Loading conversations from {file_path}...\")\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            self.conversations = json.load(file)\n",
        "\n",
        "        print(f\"‚úÖ Loaded {len(self.conversations)} conversations\")\n",
        "        return self\n",
        "\n",
        "    def extract_messages(self):\n",
        "        \"\"\"Extract messages from your chat_messages format\"\"\"\n",
        "        print(\"Extracting messages from conversations...\")\n",
        "\n",
        "        messages = []\n",
        "\n",
        "        for conv_idx, conversation in enumerate(self.conversations):\n",
        "            # Extract conversation metadata\n",
        "            conv_id = conversation.get('uuid', f'conv_{conv_idx}')\n",
        "            title = conversation.get('name', 'Untitled')\n",
        "            created_at = conversation.get('created_at', '')\n",
        "            updated_at = conversation.get('updated_at', '')\n",
        "            chat_messages = conversation.get('chat_messages', [])\n",
        "\n",
        "            # Convert timestamps\n",
        "            try:\n",
        "                if created_at:\n",
        "                    create_time = datetime.fromisoformat(created_at.replace('Z', '+00:00')).timestamp()\n",
        "                else:\n",
        "                    create_time = 0\n",
        "            except:\n",
        "                create_time = 0\n",
        "\n",
        "            try:\n",
        "                if updated_at:\n",
        "                    update_time = datetime.fromisoformat(updated_at.replace('Z', '+00:00')).timestamp()\n",
        "                else:\n",
        "                    update_time = 0\n",
        "            except:\n",
        "                update_time = 0\n",
        "\n",
        "            # Extract messages from chat_messages array\n",
        "            for msg_idx, message in enumerate(chat_messages):\n",
        "                # Extract role - convert 'human' to 'user' for consistency\n",
        "                sender = message.get('sender', 'unknown')\n",
        "                if sender == 'human':\n",
        "                    role = 'user'\n",
        "                elif sender == 'assistant' or sender == 'ai':\n",
        "                    role = 'assistant'\n",
        "                else:\n",
        "                    role = sender\n",
        "\n",
        "                # Extract content - handle complex content structure\n",
        "                content_raw = message.get('content', '')\n",
        "                text_content = \"\"\n",
        "\n",
        "                if isinstance(content_raw, list):\n",
        "                    # Extract text from list of content objects\n",
        "                    for item in content_raw:\n",
        "                        if isinstance(item, dict):\n",
        "                            # Look for text in various possible fields\n",
        "                            if 'text' in item:\n",
        "                                text_content += str(item['text']) + \" \"\n",
        "                            elif 'content' in item:\n",
        "                                text_content += str(item['content']) + \" \"\n",
        "                        else:\n",
        "                            text_content += str(item) + \" \"\n",
        "                elif isinstance(content_raw, dict):\n",
        "                    # Single content object\n",
        "                    if 'text' in content_raw:\n",
        "                        text_content = str(content_raw['text'])\n",
        "                    elif 'content' in content_raw:\n",
        "                        text_content = str(content_raw['content'])\n",
        "                    else:\n",
        "                        text_content = str(content_raw)\n",
        "                else:\n",
        "                    # Simple string content\n",
        "                    text_content = str(content_raw)\n",
        "\n",
        "                # Also check the 'text' field directly\n",
        "                if 'text' in message and message['text']:\n",
        "                    if not text_content.strip():\n",
        "                        text_content = str(message['text'])\n",
        "\n",
        "                # Get message timestamp\n",
        "                msg_created_at = message.get('created_at', created_at)\n",
        "                try:\n",
        "                    if msg_created_at:\n",
        "                        msg_time = datetime.fromisoformat(msg_created_at.replace('Z', '+00:00')).timestamp()\n",
        "                    else:\n",
        "                        msg_time = create_time\n",
        "                except:\n",
        "                    msg_time = create_time\n",
        "\n",
        "                # Only add messages with actual text content\n",
        "                if text_content and text_content.strip():\n",
        "                    msg_data = {\n",
        "                        'conversation_id': conv_id,\n",
        "                        'conversation_title': title,\n",
        "                        'conversation_create_time': create_time,\n",
        "                        'conversation_update_time': update_time,\n",
        "                        'message_id': f\"{conv_id}_{msg_idx}\",\n",
        "                        'author_role': role,\n",
        "                        'content': text_content.strip(),\n",
        "                        'message_create_time': msg_time,\n",
        "                        'content_type': 'text',\n",
        "                        'word_count': len(text_content.strip().split()),\n",
        "                        'char_count': len(text_content.strip())\n",
        "                    }\n",
        "                    messages.append(msg_data)\n",
        "\n",
        "        self.messages_df = pd.DataFrame(messages)\n",
        "        print(f\"‚úÖ Extracted {len(messages)} messages from {len(self.conversations)} conversations\")\n",
        "\n",
        "        if not self.messages_df.empty:\n",
        "            self._add_derived_features()\n",
        "        else:\n",
        "            self.user_messages_df = pd.DataFrame()\n",
        "            print(\"‚ùå No messages extracted\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _add_derived_features(self):\n",
        "        \"\"\"Add derived features to the messages dataframe\"\"\"\n",
        "        # Convert timestamps\n",
        "        self.messages_df['create_datetime'] = pd.to_datetime(\n",
        "            self.messages_df['message_create_time'], unit='s', utc=True\n",
        "        )\n",
        "        self.messages_df['conversation_create_datetime'] = pd.to_datetime(\n",
        "            self.messages_df['conversation_create_time'], unit='s', utc=True\n",
        "        )\n",
        "\n",
        "        # Extract date components\n",
        "        self.messages_df['date'] = self.messages_df['create_datetime'].dt.date\n",
        "        self.messages_df['hour'] = self.messages_df['create_datetime'].dt.hour\n",
        "        self.messages_df['day_of_week'] = self.messages_df['create_datetime'].dt.day_name()\n",
        "        self.messages_df['month'] = self.messages_df['create_datetime'].dt.month\n",
        "        self.messages_df['year'] = self.messages_df['create_datetime'].dt.year\n",
        "\n",
        "        # Filter to user messages\n",
        "        self.user_messages_df = self.messages_df[\n",
        "            self.messages_df['author_role'] == 'user'\n",
        "        ].copy()\n",
        "\n",
        "        print(f\"   Total messages: {len(self.messages_df)}\")\n",
        "        print(f\"   Your messages: {len(self.user_messages_df)}\")\n",
        "        print(f\"   Role distribution: {dict(self.messages_df['author_role'].value_counts())}\")\n",
        "\n",
        "    def _get_message_context(self, message_row):\n",
        "        \"\"\"Get context for a message\"\"\"\n",
        "        conv_id = message_row['conversation_id']\n",
        "        msg_time = message_row['message_create_time']\n",
        "\n",
        "        conv_messages = self.messages_df[\n",
        "            (self.messages_df['conversation_id'] == conv_id) &\n",
        "            (self.messages_df['message_create_time'] <= msg_time)\n",
        "        ].sort_values('message_create_time')\n",
        "\n",
        "        context_messages = conv_messages.tail(10)\n",
        "\n",
        "        context = []\n",
        "        for _, ctx_msg in context_messages.iterrows():\n",
        "            role = ctx_msg['author_role']\n",
        "            content = ctx_msg['content'][:1000]\n",
        "            context.append(f\"[{role}]: {content}\")\n",
        "\n",
        "        return \"\\n\".join(context)\n",
        "\n",
        "    def classify_messages(self, sample_size=None):\n",
        "        \"\"\"Classify messages using the paper's methodology\"\"\"\n",
        "        if len(self.user_messages_df) == 0:\n",
        "            print(\"‚ùå No user messages to classify\")\n",
        "            return self\n",
        "\n",
        "        print(\"Starting message classification...\")\n",
        "\n",
        "        # Report which method is being used\n",
        "        if self.client:\n",
        "            print(\"ü§ñ Using OpenAI API for accurate classifications\")\n",
        "        else:\n",
        "            print(\"üìù Using heuristic keyword-based classifications\")\n",
        "\n",
        "        if sample_size and len(self.user_messages_df) > sample_size:\n",
        "            messages_to_classify = self.user_messages_df.sample(sample_size, random_state=42)\n",
        "            print(f\"üìä Sampling {sample_size} messages for classification\")\n",
        "        else:\n",
        "            messages_to_classify = self.user_messages_df.copy()\n",
        "\n",
        "        classified_results = []\n",
        "        api_calls_made = 0\n",
        "\n",
        "        for idx, row in messages_to_classify.iterrows():\n",
        "            if len(classified_results) % 50 == 0:\n",
        "                print(f\"   Classifying message {len(classified_results) + 1}/{len(messages_to_classify)}...\")\n",
        "\n",
        "            context = self._get_message_context(row)\n",
        "\n",
        "            # Track API usage\n",
        "            pre_client_state = self.client is not None\n",
        "\n",
        "            classifications = {\n",
        "                'message_idx': idx,\n",
        "                'conversation_id': row['conversation_id'],\n",
        "                'message_id': row['message_id'],\n",
        "                'content': row['content'][:200] + '...' if len(row['content']) > 200 else row['content'],\n",
        "                'create_datetime': row['create_datetime'],\n",
        "                'word_count': row['word_count'],\n",
        "                'is_work': self._classify_work_related(context),\n",
        "                'intent': self._classify_intent(context),\n",
        "                'topic': self._classify_topic(context)\n",
        "            }\n",
        "\n",
        "            if pre_client_state and self.client:\n",
        "                api_calls_made += 3  # 3 classifications per message\n",
        "\n",
        "            classified_results.append(classifications)\n",
        "\n",
        "        self.classified_messages = pd.DataFrame(classified_results)\n",
        "\n",
        "        if api_calls_made > 0:\n",
        "            print(f\"‚úÖ Completed classification of {len(classified_results)} messages using OpenAI API ({api_calls_made} API calls)\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Completed classification of {len(classified_results)} messages using heuristics\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def generate_analysis(self):\n",
        "        \"\"\"Generate comprehensive analysis\"\"\"\n",
        "        if self.classified_messages is None or len(self.classified_messages) == 0:\n",
        "            print(\"‚ùå No classified messages available. Run classify_messages() first.\")\n",
        "            return\n",
        "\n",
        "        print(\"üìä Generating analysis report...\")\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "        # Work vs Non-work\n",
        "        work_counts = self.classified_messages['is_work'].value_counts()\n",
        "        axes[0,0].pie([work_counts.get(0, 0), work_counts.get(1, 0)],\n",
        "                     labels=['Non-Work', 'Work'], autopct='%1.1f%%')\n",
        "        axes[0,0].set_title('Work vs Non-Work Messages')\n",
        "\n",
        "        # Intent distribution\n",
        "        intent_counts = self.classified_messages['intent'].value_counts()\n",
        "        axes[0,1].bar(intent_counts.index, intent_counts.values)\n",
        "        axes[0,1].set_title('User Intent Distribution')\n",
        "        axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Topic distribution\n",
        "        topic_counts = self.classified_messages['topic'].value_counts()\n",
        "        axes[0,2].bar(topic_counts.index, topic_counts.values)\n",
        "        axes[0,2].set_title('Topic Distribution')\n",
        "        axes[0,2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Hourly usage\n",
        "        hourly_counts = self.user_messages_df.groupby('hour').size()\n",
        "        axes[1,0].bar(hourly_counts.index, hourly_counts.values)\n",
        "        axes[1,0].set_title('Usage by Hour')\n",
        "\n",
        "        # Daily usage\n",
        "        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "        daily_counts = self.user_messages_df.groupby('day_of_week').size().reindex(day_order, fill_value=0)\n",
        "        axes[1,1].bar(daily_counts.index, daily_counts.values)\n",
        "        axes[1,1].set_title('Usage by Day of Week')\n",
        "        axes[1,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Message length\n",
        "        axes[1,2].hist(self.user_messages_df['word_count'], bins=30, alpha=0.7)\n",
        "        axes[1,2].set_title('Message Length Distribution')\n",
        "        axes[1,2].set_xlabel('Word Count')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        self._print_summary()\n",
        "\n",
        "    def _print_summary(self):\n",
        "        \"\"\"Print summary statistics\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"CHATGPT PERSONAL USAGE ANALYSIS SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        print(f\"\\nüìä BASIC STATISTICS:\")\n",
        "        print(f\"   Total Conversations: {len(self.conversations):,}\")\n",
        "        print(f\"   Total Messages: {len(self.messages_df):,}\")\n",
        "        print(f\"   Your Messages: {len(self.user_messages_df):,}\")\n",
        "        print(f\"   Classified Messages: {len(self.classified_messages):,}\")\n",
        "\n",
        "        if len(self.user_messages_df) > 0:\n",
        "            start_date = self.user_messages_df['create_datetime'].min()\n",
        "            end_date = self.user_messages_df['create_datetime'].max()\n",
        "            print(f\"   Time Range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "            avg_length = self.user_messages_df['word_count'].mean()\n",
        "            print(f\"   Average Message Length: {avg_length:.1f} words\")\n",
        "\n",
        "        if len(self.classified_messages) > 0:\n",
        "            print(f\"\\nüè¢ WORK VS NON-WORK:\")\n",
        "            work_stats = self.classified_messages['is_work'].value_counts(normalize=True) * 100\n",
        "            print(f\"   Work-Related: {work_stats.get(1, 0):.1f}%\")\n",
        "            print(f\"   Non-Work: {work_stats.get(0, 0):.1f}%\")\n",
        "\n",
        "            print(f\"\\nüéØ INTENT DISTRIBUTION:\")\n",
        "            intent_stats = self.classified_messages['intent'].value_counts(normalize=True) * 100\n",
        "            for intent, pct in intent_stats.items():\n",
        "                print(f\"   {intent}: {pct:.1f}%\")\n",
        "\n",
        "            print(f\"\\nüìö TOPIC DISTRIBUTION:\")\n",
        "            topic_stats = self.classified_messages['topic'].value_counts(normalize=True) * 100\n",
        "            for topic, pct in topic_stats.items():\n",
        "                print(f\"   {topic}: {pct:.1f}%\")\n",
        "\n",
        "            print(f\"\\nüìà COMPARISON TO RESEARCH PAPER:\")\n",
        "            your_work_pct = work_stats.get(1, 0)\n",
        "            print(f\"   Your work usage: {your_work_pct:.1f}% (Paper average: ~27%)\")\n",
        "\n",
        "            if 'Asking' in intent_stats:\n",
        "                your_asking_pct = intent_stats.get('Asking', 0)\n",
        "                print(f\"   Your 'Asking' usage: {your_asking_pct:.1f}% (Paper average: ~49%)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "print(\"‚úÖ FIXED analyzer loaded with improved OpenAI API handling!\")\n",
        "print(\"\\nKey improvements:\")\n",
        "print(\"- Better API key detection from multiple sources\")\n",
        "print(\"- Improved error handling and fallback logic\")\n",
        "print(\"- Context truncation to avoid token limits\")\n",
        "print(\"- API usage tracking and reporting\")\n",
        "print(\"- More detailed initialization messages\")\n",
        "print(\"\\nNow run:\")\n",
        "print(\"analyzer = FixedCustomFormatAnalyzer()\")\n",
        "print(\"analyzer.load_conversations('conversations.json')\")\n",
        "print(\"analyzer.extract_messages()\")\n",
        "print(\"analyzer.classify_messages(sample_size=500)\")\n",
        "print(\"analyzer.generate_analysis()\")"
      ],
      "metadata": {
        "id": "tac4JbVVXtmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize with API key\n",
        "analyzer = FixedCustomFormatAnalyzer(userdata.get('OPENAI_API_KEY'))\n",
        "analyzer.load_conversations('conversations.json')\n",
        "analyzer.extract_messages()\n",
        "analyzer.classify_messages(sample_size=6000)\n",
        "analyzer.generate_analysis()"
      ],
      "metadata": {
        "id": "wFwElu4yWE3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g96ATKqaFMty"
      }
    }
  ]
}